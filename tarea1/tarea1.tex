
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font than Computer Modern for most use cases
    \usepackage{palatino}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Tarea \# 1 Máquinas de Aprendizaje}
    	\author{Rafik Mas'ad\\Alejandro Sazo}
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Tarea 1 Máquinas de
Aprendizaje}\label{tarea-1-muxe1quinas-de-aprendizaje}

    \subsection{Ejercicio 1}\label{ejercicio-1}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Construcción del dataframe. La primera columna del dataframe original
  es redundante para la indexación, mientras que la columna Train nos
  permite identificar cuales ejemplos serán parte del training set
  (Train = T) y del testing set (Train = F). Para explicitar qué
  ejemplos son del testing set se invierten los valores de verdad de
  dicha columna. Finalmente la columna ya utilizada se descarta para
  quedarnos con las columnas de predictores.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k+kn}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k+kn}{as} \PY{n+nn}{pd}
        
        \PY{n}{url} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{http://statweb.stanford.edu/\PYZti{}tibs/ElemStatLearn/datasets/prostate.data}\PY{l+s+s1}{\PYZsq{}}
        \PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{n}{url}\PY{p}{,} \PY{n}{sep}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{header}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Remover columna con indices redundantes}
        \PY{n}{df} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Unnamed: 0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Obtener columna con la etiqueta Train y reemplazar valores booleanos. Estos ejemplos seran de entrenamiento}
        \PY{n}{istrain\PYZus{}str} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{istrain} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{p}{[}\PY{n+nb+bp}{True} \PY{k}{if} \PY{n}{s} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{T}\PY{l+s+s1}{\PYZsq{}} \PY{k}{else} \PY{n+nb+bp}{False} \PY{k}{for} \PY{n}{s} \PY{o+ow}{in} \PY{n}{istrain\PYZus{}str}\PY{p}{]}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Listar como testing el resto de valores falsos de la columna anterior}
        \PY{n}{istest} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{logical\PYZus{}not}\PY{p}{(}\PY{n}{istrain}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Una vez procesado los datos, eliminar la columna train para almacenar los predictores relevantes}
        \PY{n}{df} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Descripción del dataset. El dataset posee 9 atributos y 97 samples con
  valores enteros y reales.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{n}{df}\PY{o}{.}\PY{n}{shape}
        \PY{n}{df}\PY{o}{.}\PY{n}{info}\PY{p}{(}\PY{p}{)}
        \PY{n}{df}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
<class 'pandas.core.frame.DataFrame'>
Int64Index: 97 entries, 0 to 96
Data columns (total 9 columns):
lcavol     97 non-null float64
lweight    97 non-null float64
age        97 non-null int64
lbph       97 non-null float64
svi        97 non-null int64
lcp        97 non-null float64
gleason    97 non-null int64
pgg45      97 non-null int64
lpsa       97 non-null float64
dtypes: float64(5), int64(4)
memory usage: 7.6 KB

    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}2}]:}           lcavol    lweight        age       lbph        svi        lcp  \textbackslash{}
        count  97.000000  97.000000  97.000000  97.000000  97.000000  97.000000   
        mean    1.350010   3.628943  63.865979   0.100356   0.216495  -0.179366   
        std     1.178625   0.428411   7.445117   1.450807   0.413995   1.398250   
        min    -1.347074   2.374906  41.000000  -1.386294   0.000000  -1.386294   
        25\%     0.512824   3.375880  60.000000  -1.386294   0.000000  -1.386294   
        50\%     1.446919   3.623007  65.000000   0.300105   0.000000  -0.798508   
        75\%     2.127041   3.876396  68.000000   1.558145   0.000000   1.178655   
        max     3.821004   4.780383  79.000000   2.326302   1.000000   2.904165   
        
                 gleason       pgg45       lpsa  
        count  97.000000   97.000000  97.000000  
        mean    6.752577   24.381443   2.478387  
        std     0.722134   28.204035   1.154329  
        min     6.000000    0.000000  -0.430783  
        25\%     6.000000    0.000000   1.731656  
        50\%     7.000000   15.000000   2.591516  
        75\%     7.000000   40.000000   3.056357  
        max     9.000000  100.000000   5.582932  
\end{Verbatim}
        
    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Normalización de datos. Este preprocesamiento de los datos es
  importante pues las features originales pueden venir en distintas
  escalas por lo tanto nuestro algoritmo de aprendizaje no funcionará
  correctamente, por ejemplo al utilizar funciones objetivo que incluyan
  métricas, los datos con mayor rango tenderán a dominar sobre los de
  menor rango; por otra parte también podría darse el caso de que la
  convergencia en algoritmos que usen gradiente descendiente sea lenta o
  imprecisa.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.preprocessing} \PY{k+kn}{import} \PY{n}{StandardScaler}
        \PY{c+c1}{\PYZsh{} Por defecto centra y escala la data.}
        \PY{n}{scaler} \PY{o}{=} \PY{n}{StandardScaler}\PY{p}{(}\PY{n}{with\PYZus{}mean}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{with\PYZus{}std}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
        \PY{n}{df\PYZus{}scaled} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{scaler}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{df}\PY{p}{)}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{df}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
        \PY{c+c1}{\PYZsh{}  Deseamos aprender a predecir el feature lpsa, por lo que la recuperamos del original}
        \PY{n}{df\PYZus{}scaled}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lpsa}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lpsa}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{3}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Regresión lineal con Mínimos Cuadrados. En primer lugar extraemos la
  última columna de los datos, que corresponde al output \(y\) de cada
  ejemplo. La nueva columna añadida corresponde al bias. El argumento
  pasado al constructor de LinearRegression indica que no se calculará
  intercepto para el modelo (ya lo hemos hecho a través de normalizar e
  ingresar la columna con bias 1)
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k+kn}{import} \PY{n+nn}{sklearn.linear\PYZus{}model} \PY{k+kn}{as} \PY{n+nn}{lm}
        \PY{n}{X} \PY{o}{=} \PY{n}{df\PYZus{}scaled}\PY{o}{.}\PY{n}{ix}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{c+c1}{\PYZsh{} Agregamos la columna de bias con 1}
        \PY{n}{N} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{X}\PY{o}{.}\PY{n}{insert}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{intercept}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{n}{N}\PY{p}{)}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Obtener los datos de output conocidos y extraer test \PYZam{} training set}
        \PY{n}{y} \PY{o}{=} \PY{n}{df\PYZus{}scaled}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lpsa}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{Xtrain} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{istrain}\PY{p}{]}
        \PY{n}{ytrain} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{istrain}\PY{p}{]}
        \PY{n}{Xtest} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{istest}\PY{p}{]}
        \PY{n}{ytest} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{istest}\PY{p}{]}
        \PY{n}{linreg} \PY{o}{=} \PY{n}{lm}\PY{o}{.}\PY{n}{LinearRegression}\PY{p}{(}\PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
        \PY{n}{linreg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xtrain}\PY{p}{,} \PY{n}{ytrain}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}4}]:} LinearRegression(copy\_X=True, fit\_intercept=False, n\_jobs=1, normalize=False)
\end{Verbatim}
        
    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{4}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Tabla de pesos (coeficientes) y Z-scores para cada variable. Los
  Z-scores miden el efecto de descartar alguna variable del modelo.
  Cuando deseamos un 5\% de significancia, las variables que resultarán
  más importantes deberán tener Z-score en valor abosluto mayor que 2,
  lo que indica una significancia del 5\%. Las variables mas importante
  por lo tanto son en orden de Z-score decreciente \textbf{lcavol},
  \textbf{svi}, \textbf{lcp} y \textbf{pgg45}
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{c+c1}{\PYZsh{} Correlacion entre variables}
        \PY{c+c1}{\PYZsh{} print Xtrain.drop(\PYZsq{}intercept\PYZsq{}, axis=1).corr()}
        
        \PY{c+c1}{\PYZsh{} Tabla con coefficientes y sus Z\PYZhy{}score}
        \PY{n}{coeffs} \PY{o}{=} \PY{n}{linreg}\PY{o}{.}\PY{n}{coef\PYZus{}}
        \PY{n}{Table} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{coeffs}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Coefficent}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        \PY{n}{Table}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Std. Error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{Xtrain}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{Xtrain}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        \PY{n}{Table}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Z\PYZhy{}score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{Table}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Coefficent}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{div}\PY{p}{(}\PY{n}{Table}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Std. Error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
        \PY{n}{Table}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}5}]:}            Coefficent  Std. Error   Z-score
        lcavol       0.676016    0.129469  5.221460
        lweight      0.261694    0.136618  1.915519
        age         -0.140734    0.123746 -1.137281
        lbph         0.209061    0.123892  1.687447
        svi          0.303623    0.124582  2.437133
        lcp         -0.287002    0.123022 -2.332923
        gleason     -0.021195    0.120547 -0.175822
        pgg45        0.265576    0.127584  2.081583
        intercept    2.464933    0.000000       inf
\end{Verbatim}
        
    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{5}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Estimación de errores de predicción. El uso de cross validation nos
  permitirá entender que tan bien generaliza nuestra máquina mientras no
  tengamos disponible el testing set. La máquina entrenando con 5 folds
  presenta un MSE (Mean squared error o error cuadrático medio) de 0.95,
  mientras que con 10 folds el MSE disminuye a 0.75. El MSE en el
  testing set es finalmente de 0.5, lo cual es positivo pues nuestro
  modelo original tenía un alto error en el training set, pero las
  pruebas sobre el testing set fueron mejores.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{c+c1}{\PYZsh{} Predecir en el testing set}
        \PY{n}{yhat\PYZus{}test} \PY{o}{=} \PY{n}{linreg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{Xtest}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Calcular error cuadrático medio en el testing set}
        \PY{n}{mse\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{yhat\PYZus{}test} \PY{o}{\PYZhy{}} \PY{n}{ytest}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
        \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE para testing set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{mse\PYZus{}test}
        \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k+kn}{import} \PY{n}{cross\PYZus{}validation}
        \PY{n}{Xm} \PY{o}{=} \PY{n}{Xtrain}\PY{o}{.}\PY{n}{as\PYZus{}matrix}\PY{p}{(}\PY{p}{)}
        \PY{n}{ym} \PY{o}{=} \PY{n}{ytrain}\PY{o}{.}\PY{n}{as\PYZus{}matrix}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Definir numero de folds}
        \PY{n}{n\PYZus{}folds} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}
        \PY{c+c1}{\PYZsh{} Estimar error con 5 y 10 folds}
        \PY{k}{for} \PY{n}{nf} \PY{o+ow}{in} \PY{n}{n\PYZus{}folds}\PY{p}{:}
            \PY{n}{k\PYZus{}fold} \PY{o}{=} \PY{n}{cross\PYZus{}validation}\PY{o}{.}\PY{n}{KFold}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Xm}\PY{p}{)}\PY{p}{,} \PY{n}{nf}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} MSE para cross validation}
            \PY{n}{mse\PYZus{}cv} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{k}{for} \PY{n}{k}\PY{p}{,} \PY{p}{(}\PY{n}{train}\PY{p}{,} \PY{n}{val}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{k\PYZus{}fold}\PY{p}{)}\PY{p}{:}
                \PY{n}{linreg} \PY{o}{=} \PY{n}{lm}\PY{o}{.}\PY{n}{LinearRegression}\PY{p}{(}\PY{n}{fit\PYZus{}intercept} \PY{o}{=} \PY{n+nb+bp}{False}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} Modelar con el subconjunto del training set dado por el fold}
                \PY{n}{linreg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xm}\PY{p}{[}\PY{n}{train}\PY{p}{]}\PY{p}{,} \PY{n}{ym}\PY{p}{[}\PY{n}{train}\PY{p}{]}\PY{p}{)}
                \PY{n}{yhat\PYZus{}val} \PY{o}{=} \PY{n}{linreg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{Xm}\PY{p}{[}\PY{n}{val}\PY{p}{]}\PY{p}{)}
                \PY{n}{mse\PYZus{}fold} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{yhat\PYZus{}val} \PY{o}{\PYZhy{}} \PY{n}{ym}\PY{p}{[}\PY{n}{val}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                \PY{n}{mse\PYZus{}cv} \PY{o}{+}\PY{o}{=} \PY{n}{mse\PYZus{}fold}
            \PY{n}{mse\PYZus{}cv} \PY{o}{=} \PY{n}{mse\PYZus{}cv} \PY{o}{/} \PY{n}{nf}
            \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE para}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{nf}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{folds:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{mse\PYZus{}cv}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
MSE para testing set: 0.521274005508
MSE para 5 folds: 0.956514631616
MSE para 10 folds: 0.757237472963

    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{9}
\itemsep1pt\parskip0pt\parsep0pt
\item
  QQ Plot para error de prueba. Podemos observar que \(R^2 = 0.9537\),
  lo que indica una alta correlación entre los datos de predicción, por
  lo que es razonable suponer que los residuos se distribuyen de forma
  normal.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        \PY{k+kn}{import} \PY{n+nn}{matplotlib.pyplot} \PY{k+kn}{as} \PY{n+nn}{plt}
        \PY{k+kn}{import} \PY{n+nn}{scipy.stats} \PY{k+kn}{as} \PY{n+nn}{stats}
        \PY{n}{residual} \PY{o}{=} \PY{n}{yhat\PYZus{}test} \PY{o}{\PYZhy{}} \PY{n}{ytest}
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
        \PY{n}{stats}\PY{o}{.}\PY{n}{probplot}\PY{p}{(}\PY{n}{residual}\PY{p}{,} \PY{n}{dist}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{norm}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{plot}\PY{o}{=}\PY{n}{plt}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Q\PYZhy{}Q Plot}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Quantiles}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Residuos}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{tarea1_files/tarea1_15_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Ejercicio 2}\label{ejercicio-2}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Implementación de FSS. En vez de simplemente usar el error cuadrático
  medio como criterio para aceptar o no una variable se ha implementado
  una versión que utiliza el coeficiente \(R^2\). Gráficamente los
  errores de entrenamiento siempre decrecen, pero con 5 variables los
  errores en el conjunto de test son del orden de 0.45, añadir más
  variables aumenta el error de test a partir de ese punto
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k}{def} \PY{n+nf}{fss}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{x\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{names\PYZus{}x}\PY{p}{,} \PY{n}{k} \PY{o}{=} \PY{l+m+mi}{10000}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{        Forward Step\PYZhy{}wise Selection}
        \PY{l+s+sd}{        Args:}
        \PY{l+s+sd}{            x: Training set x}
        \PY{l+s+sd}{            y: Training set y}
        \PY{l+s+sd}{            x\PYZus{}test: Testing set x}
        \PY{l+s+sd}{            y\PYZus{}test: Testing set y}
        \PY{l+s+sd}{            names\PYZus{}x: Labels for training set x}
        \PY{l+s+sd}{            k: Max number of variables}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} Numero de features}
            \PY{n}{p} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
            \PY{n}{k} \PY{o}{=} \PY{n+nb}{min}\PY{p}{(}\PY{n}{p}\PY{p}{,} \PY{n}{k}\PY{p}{)}
            \PY{n}{names\PYZus{}x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{names\PYZus{}x}\PY{p}{)}
            \PY{n}{remaining} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{p}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} Mantener intercepto}
            \PY{n}{selected} \PY{o}{=} \PY{p}{[}\PY{n}{p}\PY{p}{]}
            \PY{n}{current\PYZus{}score} \PY{o}{=} \PY{l+m+mf}{0.0}
            \PY{n}{best\PYZus{}new\PYZus{}score} \PY{o}{=} \PY{l+m+mf}{0.0}
            \PY{n}{nvars} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{n}{training\PYZus{}errors} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{n}{test\PYZus{}errors} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{c+c1}{\PYZsh{} Mientras hayan candidatos y las variables seleccionadas no superen k}
            \PY{k}{while} \PY{n}{remaining} \PY{o+ow}{and} \PY{n+nb}{len}\PY{p}{(}\PY{n}{selected}\PY{p}{)} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{n}{k}\PY{p}{:}
                \PY{n}{score\PYZus{}candidates} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                \PY{c+c1}{\PYZsh{} Por cada variable candidata}
                \PY{k}{for} \PY{n}{candidate} \PY{o+ow}{in} \PY{n}{remaining}\PY{p}{:}
                    \PY{c+c1}{\PYZsh{} Crear un nuevo modelo de regresion}
                    \PY{n}{model} \PY{o}{=} \PY{n}{lm}\PY{o}{.}\PY{n}{LinearRegression}\PY{p}{(}\PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
                    \PY{n}{indexes} \PY{o}{=} \PY{n}{selected} \PY{o}{+} \PY{p}{[}\PY{n}{candidate}\PY{p}{]}
                    \PY{c+c1}{\PYZsh{} Extraer como conjunto de entrenamiento el intercepto }
                    \PY{c+c1}{\PYZsh{} y los valores asociados a las variables elegidas}
                    \PY{n}{x\PYZus{}train} \PY{o}{=} \PY{n}{x}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{indexes}\PY{p}{]}
                    \PY{c+c1}{\PYZsh{} Hacer el fit del modelo y predecir}
                    \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y}\PY{p}{)}
                    \PY{c+c1}{\PYZsh{} Predecir sobre training y test}
                    \PY{n}{\PYZus{}x\PYZus{}test} \PY{o}{=} \PY{n}{x\PYZus{}test}\PY{o}{.}\PY{n}{as\PYZus{}matrix}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{indexes}\PY{p}{]}
                    \PY{n}{yhat\PYZus{}train} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{)}
                    \PY{n}{yhat\PYZus{}test} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{\PYZus{}x\PYZus{}test}\PY{p}{)}
                    \PY{c+c1}{\PYZsh{} Obtener residuos y calcular R\PYZca{}2}
                    \PY{n}{residuals\PYZus{}train} \PY{o}{=} \PY{n}{yhat\PYZus{}train} \PY{o}{\PYZhy{}} \PY{n}{y}
                    \PY{n}{residuals\PYZus{}test} \PY{o}{=} \PY{n}{yhat\PYZus{}test} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}test}
                    \PY{c+c1}{\PYZsh{} Calcular Error de entrenamiento}
                    \PY{n}{training\PYZus{}error} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{residuals\PYZus{}train}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                    \PY{n}{test\PYZus{}error} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{residuals\PYZus{}test}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                    \PY{n}{mean\PYZus{}y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{y}\PY{p}{)}
                    \PY{n}{SS\PYZus{}tot} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{mean\PYZus{}y}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                    \PY{n}{SS\PYZus{}res} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{residuals\PYZus{}train}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                    \PY{n}{R2} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{n}{SS\PYZus{}res}\PY{o}{/}\PY{n}{SS\PYZus{}tot}\PY{p}{)}
                    \PY{n}{score\PYZus{}candidates}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{(}\PY{n}{R2}\PY{p}{,} \PY{n}{candidate}\PY{p}{,} \PY{n}{training\PYZus{}error}\PY{p}{,} \PY{n}{test\PYZus{}error}\PY{p}{)}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} Una vez analizadas las candidatas ordenar scores de mayor a menor}
                \PY{n}{score\PYZus{}candidates}\PY{o}{.}\PY{n}{sort}\PY{p}{(}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} Extraer el elemento de mejor score}
                \PY{n}{best\PYZus{}new\PYZus{}score}\PY{p}{,} \PY{n}{best\PYZus{}candidate}\PY{p}{,} \PY{n}{best\PYZus{}training\PYZus{}error}\PY{p}{,} \PY{n}{best\PYZus{}test\PYZus{}error}\PY{o}{=} \PY{n}{score\PYZus{}candidates}\PY{o}{.}\PY{n}{pop}\PY{p}{(}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} Remover al candidato de la lista de restantes y agregarlo a la lista de seleccionados}
                \PY{n}{remaining}\PY{o}{.}\PY{n}{remove}\PY{p}{(}\PY{n}{best\PYZus{}candidate}\PY{p}{)}
                \PY{n}{selected}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{best\PYZus{}candidate}\PY{p}{)}
                \PY{n}{nvars}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{indexes}\PY{p}{)}\PY{p}{)}
                \PY{n}{training\PYZus{}errors}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{best\PYZus{}training\PYZus{}error}\PY{p}{)}
                \PY{n}{test\PYZus{}errors}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{best\PYZus{}test\PYZus{}error}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} R\PYZca{}2 correspondiente a esta configuracion para testing set}
                \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{selected = }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s2}{ ...}\PY{l+s+s2}{\PYZdq{}}\PY{o}{\PYZpc{}}\PY{k}{names\PYZus{}x}[best\PYZus{}candidate]
                \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{totalvars=}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{, R\PYZca{}2 = }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}}\PY{o}{\PYZpc{}}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{indexes}\PY{p}{)}\PY{p}{,}\PY{n}{best\PYZus{}new\PYZus{}score}\PY{p}{)}
            \PY{k}{return} \PY{n}{selected}\PY{p}{,} \PY{n}{nvars}\PY{p}{,} \PY{n}{training\PYZus{}errors}\PY{p}{,} \PY{n}{test\PYZus{}errors}
        
        \PY{n}{names\PYZus{}regressors} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lcavol}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lweight}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Age}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lbph}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Svi}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lcp}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Gleason}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Pgg45}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
        \PY{n}{selected}\PY{p}{,} \PY{n}{nvars}\PY{p}{,} \PY{n}{training\PYZus{}errors}\PY{p}{,} \PY{n}{test\PYZus{}errors} \PY{o}{=} \PY{n}{fss}\PY{p}{(}\PY{n}{Xm}\PY{p}{,} \PY{n}{ym}\PY{p}{,} \PY{n}{Xtest}\PY{p}{,} \PY{n}{ytest}\PY{p}{,} \PY{n}{names\PYZus{}regressors}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{nvars}\PY{p}{,} \PY{n}{training\PYZus{}errors}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bo\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{nvars}\PY{p}{,} \PY{n}{test\PYZus{}errors}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{} variables}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
selected = Lcavol {\ldots}
totalvars=2, R\^{}2 = 0.537516
selected = Lweight {\ldots}
totalvars=3, R\^{}2 = 0.614756
selected = Svi {\ldots}
totalvars=4, R\^{}2 = 0.637441
selected = Lbph {\ldots}
totalvars=5, R\^{}2 = 0.659176
selected = Pgg45 {\ldots}
totalvars=6, R\^{}2 = 0.666920
selected = Lcp {\ldots}
totalvars=7, R\^{}2 = 0.682807
selected = Age {\ldots}
totalvars=8, R\^{}2 = 0.694258
selected = Gleason {\ldots}
totalvars=9, R\^{}2 = 0.694371

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{tarea1_files/tarea1_18_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Implementación de BSS. Se mantiene la forma de calcular el score
  anterior. En este caso los resultados obtenidos son consistentes con
  la implementación de FSS. Con este dataset y score es posible apreciar
  que ambos algoritmos llegan a una zona donde los errores de testing
  son inferiores a 0.5 y menores a los errores de entrenamiento (modelo
  2 a 5 variables).
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{k}{def} \PY{n+nf}{bss}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{x\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{names\PYZus{}x}\PY{p}{)}\PY{p}{:}
            \PY{n}{names\PYZus{}x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{names\PYZus{}x}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} Inicialmente no descartamos ningun valor}
            \PY{n}{dropped} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{c+c1}{\PYZsh{} Las variables del modelo son todas las originales}
            \PY{n}{variables} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{x}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
            \PY{n}{nvars} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{n}{training\PYZus{}errors} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{n}{test\PYZus{}errors} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{c+c1}{\PYZsh{} Mientras hayan variables}
            \PY{k}{while} \PY{n+nb}{len}\PY{p}{(}\PY{n}{variables}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{1}\PY{p}{:}
                \PY{n}{scores} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                \PY{c+c1}{\PYZsh{} Por cada variable verificar cual es aquella que menos influye}
                \PY{k}{for} \PY{n}{candidate} \PY{o+ow}{in} \PY{n}{variables}\PY{p}{:}
                    \PY{c+c1}{\PYZsh{} Crear un nuevo modelo de regresion}
                    \PY{n}{model} \PY{o}{=} \PY{n}{lm}\PY{o}{.}\PY{n}{LinearRegression}\PY{p}{(}\PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
                    \PY{n}{indexes} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                    \PY{n}{indexes}\PY{p}{[}\PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{n}{variables} \PY{o}{+} \PY{p}{[}\PY{n}{x}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
                    \PY{c+c1}{\PYZsh{} Remover una variable}
                    \PY{n}{indexes}\PY{o}{.}\PY{n}{remove}\PY{p}{(}\PY{n}{candidate}\PY{p}{)}
                    \PY{n}{x\PYZus{}train} \PY{o}{=} \PY{n}{x}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{indexes}\PY{p}{]}
                    \PY{c+c1}{\PYZsh{} Hacer el fit del modelo y predecir}
                    \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y}\PY{p}{)}
                    \PY{c+c1}{\PYZsh{} Predecir sobre training y test}
                    \PY{n}{\PYZus{}x\PYZus{}test} \PY{o}{=} \PY{n}{x\PYZus{}test}\PY{o}{.}\PY{n}{as\PYZus{}matrix}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{indexes}\PY{p}{]}
                    \PY{n}{yhat\PYZus{}test} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{\PYZus{}x\PYZus{}test}\PY{p}{)}
                    \PY{n}{yhat\PYZus{}train} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{)}
                    \PY{c+c1}{\PYZsh{} Obtener residuos y calcular R\PYZca{}2}
                    \PY{n}{residuals\PYZus{}train} \PY{o}{=} \PY{n}{yhat\PYZus{}train} \PY{o}{\PYZhy{}} \PY{n}{y}
                    \PY{n}{residuals\PYZus{}test} \PY{o}{=} \PY{n}{yhat\PYZus{}test} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}test}
                    \PY{c+c1}{\PYZsh{} Calcular Error de entrenamiento}
                    \PY{n}{training\PYZus{}error} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{residuals\PYZus{}train}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                    \PY{n}{test\PYZus{}error} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{residuals\PYZus{}test}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                    \PY{n}{mean\PYZus{}y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{y}\PY{p}{)}
                    \PY{n}{SS\PYZus{}tot} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{mean\PYZus{}y}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                    \PY{n}{SS\PYZus{}res} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{residuals\PYZus{}train}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                    \PY{c+c1}{\PYZsh{} Calcular R\PYZca{}2 para cada modelo}
                    \PY{n}{R2} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{n}{SS\PYZus{}res}\PY{o}{/}\PY{n}{SS\PYZus{}tot}\PY{p}{)}
                    \PY{n}{scores}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{(}\PY{n}{R2}\PY{p}{,} \PY{n}{candidate}\PY{p}{,} \PY{n}{training\PYZus{}error}\PY{p}{,} \PY{n}{test\PYZus{}error}\PY{p}{)}\PY{p}{)}
                \PY{n}{scores}\PY{o}{.}\PY{n}{sort}\PY{p}{(}\PY{p}{)}
                \PY{n}{best\PYZus{}new\PYZus{}score}\PY{p}{,} \PY{n}{drop\PYZus{}candidate}\PY{p}{,} \PY{n}{best\PYZus{}training\PYZus{}error}\PY{p}{,} \PY{n}{best\PYZus{}test\PYZus{}error} \PY{o}{=} \PY{n}{scores}\PY{o}{.}\PY{n}{pop}\PY{p}{(}\PY{p}{)}
                \PY{c+c1}{\PYZsh{}score\PYZus{}dropped[:] = score\PYZus{}dropped[::\PYZhy{}1]}
                \PY{c+c1}{\PYZsh{}worst\PYZus{}new\PYZus{}score, worst\PYZus{}candidate, worst\PYZus{}training\PYZus{}error, worst\PYZus{}test\PYZus{}error = score\PYZus{}dropped.pop()}
                \PY{n}{variables}\PY{o}{.}\PY{n}{remove}\PY{p}{(}\PY{n}{drop\PYZus{}candidate}\PY{p}{)}
                \PY{n}{dropped}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{drop\PYZus{}candidate}\PY{p}{)}
                \PY{n}{nvars}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{indexes}\PY{p}{)}\PY{p}{)}
                \PY{n}{training\PYZus{}errors}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{best\PYZus{}training\PYZus{}error}\PY{p}{)}
                \PY{n}{test\PYZus{}errors}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{best\PYZus{}test\PYZus{}error}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} R\PYZca{}2 correspondiente a esta configuracion para testing set}
                \PY{k}{if}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{variables}\PY{p}{)} \PY{o}{==} \PY{n}{x}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                    \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{No variables dropped}\PY{l+s+s2}{\PYZdq{}}
                    \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{totalvars=}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{, best R\PYZca{}2 = }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}}\PY{o}{\PYZpc{}}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{indexes}\PY{p}{)}\PY{p}{,} \PY{n}{best\PYZus{}new\PYZus{}score}\PY{p}{)}
                \PY{k}{else}\PY{p}{:}
                    \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dropped = }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s2}{ ...}\PY{l+s+s2}{\PYZdq{}}\PY{o}{\PYZpc{}}\PY{k}{names\PYZus{}x}[drop\PYZus{}candidate]
                    \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{surviving = }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s2}{ ...}\PY{l+s+s2}{\PYZdq{}}\PY{o}{\PYZpc{}}\PY{k}{names\PYZus{}x}[variables]
                    \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{totalvars=}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{, best R\PYZca{}2 = }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}}\PY{o}{\PYZpc{}}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{indexes}\PY{p}{)}\PY{p}{,} \PY{n}{best\PYZus{}new\PYZus{}score}\PY{p}{)}
            \PY{k}{return}  \PY{n}{nvars}\PY{p}{,} \PY{n}{training\PYZus{}errors}\PY{p}{,} \PY{n}{test\PYZus{}errors}
        
        \PY{n}{nvars}\PY{p}{,} \PY{n}{training\PYZus{}errors}\PY{p}{,} \PY{n}{test\PYZus{}errors} \PY{o}{=} \PY{n}{bss}\PY{p}{(}\PY{n}{Xm}\PY{p}{,} \PY{n}{ym}\PY{p}{,} \PY{n}{Xtest}\PY{p}{,} \PY{n}{ytest}\PY{p}{,} \PY{n}{names\PYZus{}regressors}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{invert\PYZus{}xaxis}\PY{p}{(}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{nvars}\PY{p}{,} \PY{n}{training\PYZus{}errors}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bo\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{nvars}\PY{p}{,} \PY{n}{test\PYZus{}errors}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{} variables}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
No variables dropped
totalvars=9, best R\^{}2 = 0.694371
dropped = Gleason {\ldots}
surviving = ['Lcavol' 'Lweight' 'Age' 'Lbph' 'Svi' 'Lcp' 'Pgg45'] {\ldots}
totalvars=8, best R\^{}2 = 0.694258
dropped = Age {\ldots}
surviving = ['Lcavol' 'Lweight' 'Lbph' 'Svi' 'Lcp' 'Pgg45'] {\ldots}
totalvars=7, best R\^{}2 = 0.682807
dropped = Lcp {\ldots}
surviving = ['Lcavol' 'Lweight' 'Lbph' 'Svi' 'Pgg45'] {\ldots}
totalvars=6, best R\^{}2 = 0.666920
dropped = Pgg45 {\ldots}
surviving = ['Lcavol' 'Lweight' 'Lbph' 'Svi'] {\ldots}
totalvars=5, best R\^{}2 = 0.659176
dropped = Lbph {\ldots}
surviving = ['Lcavol' 'Lweight' 'Svi'] {\ldots}
totalvars=4, best R\^{}2 = 0.637441
dropped = Svi {\ldots}
surviving = ['Lcavol' 'Lweight'] {\ldots}
totalvars=3, best R\^{}2 = 0.614756
dropped = Lweight {\ldots}
surviving = ['Lcavol'] {\ldots}
totalvars=2, best R\^{}2 = 0.537516

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{tarea1_files/tarea1_20_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Ejercicio 3}\label{ejercicio-3}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Ajuste un modelo lineal utilizando ``Ridge Regression'', es decir,
  regularizando con la norma L2. Utilice valores del parámetro de
  regularización lambda en el rango \([10^{-4}, 10^{-1}]\). Construya un
  gráfico que muestre los coeficientes obtenidos como función del
  parámetro de regularización. Describa lo que observa.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{X} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{intercept}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.linear\PYZus{}model} \PY{k+kn}{import} \PY{n}{Ridge}
         \PY{k+kn}{import} \PY{n+nn}{matplotlib.pylab} \PY{k+kn}{as} \PY{n+nn}{plt}
         
         \PY{n}{Xtrain} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{istrain}\PY{p}{]}
         \PY{n}{ytrain} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{istrain}\PY{p}{]}
         \PY{n}{names\PYZus{}regressors} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lcavol}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lweight}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Age}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lbph}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Svi}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lcp}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Gleason}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Pgg45}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
         \PY{n}{alphas\PYZus{}} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{base}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         \PY{n}{coefs} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{model} \PY{o}{=} \PY{n}{Ridge}\PY{p}{(}\PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,}\PY{n}{solver}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{svd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{a} \PY{o+ow}{in} \PY{n}{alphas\PYZus{}}\PY{p}{:}
             \PY{n}{model}\PY{o}{.}\PY{n}{set\PYZus{}params}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{n}{a}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xtrain}\PY{p}{,} \PY{n}{ytrain}\PY{p}{)}
             \PY{n}{coefs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{)}
         
         \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{y\PYZus{}arr}\PY{p}{,} \PY{n}{label} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{coefs}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{names\PYZus{}regressors}\PY{p}{)}\PY{p}{:}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}}\PY{p}{,} \PY{n}{y\PYZus{}arr}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n}{label}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xscale}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlim}\PY{p}{(}\PY{n}{ax}\PY{o}{.}\PY{n}{get\PYZus{}xlim}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{} reverse axis}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{alpha}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weights}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Regularization Path RIDGE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{tarea1_files/tarea1_24_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Se observa que a mayor alpha el algoritmo es más agresivo en no
considerar variables (darles peso 0). El comportamiento se estabiliza,
en este caso, en \(10^{-1}\) (se hizo experimentos con valores menores
de alpha y es muy similar el resultado).

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Ajuste un modelo lineal utilizando el método ``Lasso'', es decir,
  regularizando con la norma L1. Utilice valores del parámetro de
  regularización lambda en el rango \([10^1 , 10^{-2}]\). Para obtener
  el código, modifique las líneas 7 y 9 del ejemplo anterior. Construya
  un gráfico que muestre los coeficientes obtenidos como función del
  parámetro de regularización. Describa lo que observa. ¿Es más efectivo
  Lasso para seleccionar atributos?
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.linear\PYZus{}model} \PY{k+kn}{import} \PY{n}{Lasso}
         \PY{k+kn}{import} \PY{n+nn}{matplotlib.pylab} \PY{k+kn}{as} \PY{n+nn}{plt}
         
         \PY{n}{Xtrain} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{istrain}\PY{p}{]}
         \PY{n}{ytrain} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{istrain}\PY{p}{]}
         \PY{n}{names\PYZus{}regressors} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lcavol}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lweight}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Age}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lbph}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Svi}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lcp}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Gleason}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Pgg45}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
         \PY{n}{alphas\PYZus{}} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,}\PY{n}{base}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         \PY{n}{coefs} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{model} \PY{o}{=} \PY{n}{Lasso}\PY{p}{(}\PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{a} \PY{o+ow}{in} \PY{n}{alphas\PYZus{}}\PY{p}{:}
             \PY{n}{model}\PY{o}{.}\PY{n}{set\PYZus{}params}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{n}{a}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xtrain}\PY{p}{,} \PY{n}{ytrain}\PY{p}{)}
             \PY{n}{coefs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{)}
         
         \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{y\PYZus{}arr}\PY{p}{,} \PY{n}{label} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{coefs}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{names\PYZus{}regressors}\PY{p}{)}\PY{p}{:}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}}\PY{p}{,} \PY{n}{y\PYZus{}arr}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n}{label}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xscale}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlim}\PY{p}{(}\PY{n}{ax}\PY{o}{.}\PY{n}{get\PYZus{}xlim}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{} reverse axis}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{alpha}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weights}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Regularization Path LASSO}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{tarea1_files/tarea1_27_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Con Lasso se anulan las variables de manera más acelerada lo cual logra
que la selección del parámetro alpha se comporte, aproximadamente, como
la cantidad de variables a anular.

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Utilizando ``Ridge Regression'', construya un gráfico que muestre el
  error de entrenamiento y el error de pruebas como función del
  parámetro de regularización. Discuta lo que observa.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{Xtest} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{logical\PYZus{}not}\PY{p}{(}\PY{n}{istrain}\PY{p}{)}\PY{p}{]}
         \PY{n}{ytest} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{logical\PYZus{}not}\PY{p}{(}\PY{n}{istrain}\PY{p}{)}\PY{p}{]}
         \PY{n}{alphas\PYZus{}} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,}\PY{n}{base}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         \PY{n}{coefs} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{model} \PY{o}{=} \PY{n}{Ridge}\PY{p}{(}\PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
         \PY{n}{mse\PYZus{}test} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{mse\PYZus{}train} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         
         \PY{k}{for} \PY{n}{a} \PY{o+ow}{in} \PY{n}{alphas\PYZus{}}\PY{p}{:}
             \PY{n}{model}\PY{o}{.}\PY{n}{set\PYZus{}params}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{n}{a}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xtrain}\PY{p}{,} \PY{n}{ytrain}\PY{p}{)}
             \PY{n}{yhat\PYZus{}train} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{Xtrain}\PY{p}{)}
             \PY{n}{yhat\PYZus{}test} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{Xtest}\PY{p}{)}
             \PY{n}{mse\PYZus{}train}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{yhat\PYZus{}train} \PY{o}{\PYZhy{}} \PY{n}{ytrain}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             \PY{n}{mse\PYZus{}test}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{yhat\PYZus{}test} \PY{o}{\PYZhy{}} \PY{n}{ytest}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}}\PY{p}{,}\PY{n}{mse\PYZus{}train}\PY{p}{,}\PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train error ridge}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}}\PY{p}{,}\PY{n}{mse\PYZus{}test}\PY{p}{,}\PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test error ridge}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xscale}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlim}\PY{p}{(}\PY{n}{ax}\PY{o}{.}\PY{n}{get\PYZus{}xlim}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         
         \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El error minimo se encuentra en alpha =}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{min}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{mse\PYZus{}test}\PY{p}{,} \PY{n}{yhat\PYZus{}test}\PY{p}{)}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{tarea1_files/tarea1_30_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
El error minimo se encuentra en alpha = 2.20307927232

    \end{Verbatim}

    El error de entrenamiento de Ridge es menor a medida que disminuye el
alpha (más variables son consideradas) producto, probablemente, del
sobre ajuste. Mientras tanto el error en el conjunto de prueba encuentra
su optimo en un punto (en este caso en alpha igual a 2.2) lo que hace
latente la necesidad de usar técnicas como cross-validation para
empíricamente tener una mejor intuición de cuales son los parámetros
óptimos.

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{3}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Utilizando ``Lasso'', construya un gráfico que muestre el error de
  entrenamiento y el error de pruebas como función del parámetro de
  regularización. Discuta lo que observa.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{Xtest} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{logical\PYZus{}not}\PY{p}{(}\PY{n}{istrain}\PY{p}{)}\PY{p}{]}
         \PY{n}{ytest} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{logical\PYZus{}not}\PY{p}{(}\PY{n}{istrain}\PY{p}{)}\PY{p}{]}
         \PY{n}{alphas\PYZus{}} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,}\PY{n}{base}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         \PY{n}{coefs} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{model} \PY{o}{=} \PY{n}{Lasso}\PY{p}{(}\PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
         \PY{n}{mse\PYZus{}test} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{mse\PYZus{}train} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         
         \PY{k}{for} \PY{n}{a} \PY{o+ow}{in} \PY{n}{alphas\PYZus{}}\PY{p}{:}
             \PY{n}{model}\PY{o}{.}\PY{n}{set\PYZus{}params}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{n}{a}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xtrain}\PY{p}{,} \PY{n}{ytrain}\PY{p}{)}
             \PY{n}{yhat\PYZus{}train} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{Xtrain}\PY{p}{)}
             \PY{n}{yhat\PYZus{}test} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{Xtest}\PY{p}{)}
             \PY{n}{mse\PYZus{}train}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{yhat\PYZus{}train} \PY{o}{\PYZhy{}} \PY{n}{ytrain}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             \PY{n}{mse\PYZus{}test}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{yhat\PYZus{}test} \PY{o}{\PYZhy{}} \PY{n}{ytest}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}}\PY{p}{,}\PY{n}{mse\PYZus{}train}\PY{p}{,}\PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train error lasso}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}}\PY{p}{,}\PY{n}{mse\PYZus{}test}\PY{p}{,}\PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test error lasso}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xscale}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlim}\PY{p}{(}\PY{n}{ax}\PY{o}{.}\PY{n}{get\PYZus{}xlim}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{tarea1_files/tarea1_33_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Se observa lo mismo que el ejercicio anterior pero de forma más agresiva
producto de lo discutido en la pregunta (b)

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{4}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Estime el valor del parámetro de regularización en los métodos
  anteriores usando validación cruzada.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{k}{def} \PY{n+nf}{MSE}\PY{p}{(}\PY{n}{y}\PY{p}{,}\PY{n}{yhat}\PY{p}{)}\PY{p}{:} \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{yhat}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
         \PY{n}{Xm} \PY{o}{=} \PY{n}{Xtrain}\PY{o}{.}\PY{n}{as\PYZus{}matrix}\PY{p}{(}\PY{p}{)}
         \PY{n}{ym} \PY{o}{=} \PY{n}{ytrain}\PY{o}{.}\PY{n}{as\PYZus{}matrix}\PY{p}{(}\PY{p}{)}
         \PY{n}{k\PYZus{}fold} \PY{o}{=} \PY{n}{cross\PYZus{}validation}\PY{o}{.}\PY{n}{KFold}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Xm}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}
         \PY{n}{best\PYZus{}cv\PYZus{}mse} \PY{o}{=} \PY{n+nb}{float}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{inf}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{model} \PY{o}{=} \PY{n}{Lasso}\PY{p}{(}\PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{a} \PY{o+ow}{in} \PY{n}{alphas\PYZus{}}\PY{p}{:}
             \PY{n}{model}\PY{o}{.}\PY{n}{set\PYZus{}params}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{n}{a}\PY{p}{)}
             \PY{n}{mse\PYZus{}list\PYZus{}k10} \PY{o}{=} \PY{p}{[}\PY{n}{MSE}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xm}\PY{p}{[}\PY{n}{train}\PY{p}{]}\PY{p}{,} \PY{n}{ym}\PY{p}{[}\PY{n}{train}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{Xm}\PY{p}{[}\PY{n}{val}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{ym}\PY{p}{[}\PY{n}{val}\PY{p}{]}\PY{p}{)} \PYZbs{}
             \PY{k}{for} \PY{n}{train}\PY{p}{,} \PY{n}{val} \PY{o+ow}{in} \PY{n}{k\PYZus{}fold}\PY{p}{]}
             \PY{k}{if} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{mse\PYZus{}list\PYZus{}k10}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n}{best\PYZus{}cv\PYZus{}mse}\PY{p}{:}
                 \PY{n}{best\PYZus{}cv\PYZus{}mse} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{mse\PYZus{}list\PYZus{}k10}\PY{p}{)}
                 \PY{n}{best\PYZus{}alpha} \PY{o}{=} \PY{n}{a}
         
         \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BEST PARAMETER=}\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{, MSE(CV)=}\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}}\PY{o}{\PYZpc{}}\PY{p}{(}\PY{n}{best\PYZus{}alpha}\PY{p}{,}\PY{n}{best\PYZus{}cv\PYZus{}mse}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
BEST PARAMETER=0.010000, MSE(CV)=0.758661

    \end{Verbatim}

    \section{Ejercicio 4}\label{ejercicio-4}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Lea los archivos de datos y cárguelos en dos dataframe o matrices X,
  y. En el caso de X es extremadamente importante que mantenga el
  formato disperso (sparse) (¿porqué?).
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{k+kn}{import} \PY{n+nn}{os.path}
         \PY{n}{data\PYZus{}base\PYZus{}path} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./data}\PY{l+s+s2}{\PYZdq{}}
         \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k+kn}{as} \PY{n+nn}{pd}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k+kn}{as} \PY{n+nn}{np}
         \PY{k+kn}{from} \PY{n+nn}{scipy.sparse} \PY{k+kn}{import} \PY{n}{csr\PYZus{}matrix}
         \PY{k+kn}{from} \PY{n+nn}{scipy.io} \PY{k+kn}{import} \PY{n}{mmread}
         
         \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{csr\PYZus{}matrix}\PY{p}{(}\PY{n}{mmread}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{data\PYZus{}base\PYZus{}path}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train.x.mm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{loadtxt}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{data\PYZus{}base\PYZus{}path}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train.y.dat}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{X\PYZus{}dev} \PY{o}{=} \PY{n}{csr\PYZus{}matrix}\PY{p}{(}\PY{n}{mmread}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{data\PYZus{}base\PYZus{}path}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dev.x.mm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{y\PYZus{}dev} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{loadtxt}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{data\PYZus{}base\PYZus{}path}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dev.y.dat}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{csr\PYZus{}matrix}\PY{p}{(}\PY{n}{mmread}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{data\PYZus{}base\PYZus{}path}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test.x.mm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{loadtxt}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{data\PYZus{}base\PYZus{}path}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test.y.dat}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    Se usa una matriz sparse ya que, producto de que hay muchos datos vacios
(normal en texto). Así se comprime la matriz y queda en un tamaño
manejable.

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Construya un modelo lineal que obtenga un coeficiente de determinación
  (sobre el conjunto de pruebas) de al menos 0.75.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{get\PYZus{}shape}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}17}]:} (1147, 145256)
\end{Verbatim}
        
    Se observa que hay 1147 peliculas cada una con 145256 atributos.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{c+c1}{\PYZsh{}from sklearn.preprocessing import StandardScaler}
         
         \PY{c+c1}{\PYZsh{}x\PYZus{}scaler = StandardScaler(with\PYZus{}mean=False)}
         
         \PY{c+c1}{\PYZsh{}X\PYZus{}train = x\PYZus{}scaler.fit\PYZus{}transform(X\PYZus{}train, y\PYZus{}train)}
         \PY{c+c1}{\PYZsh{}X\PYZus{}test = x\PYZus{}scaler.transform(X\PYZus{}test, y\PYZus{}test)}
\end{Verbatim}

    Se prueba escalar los datos pero esto devuelve peores resultados.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.feature\PYZus{}selection} \PY{k+kn}{import} \PY{n}{SelectKBest}\PY{p}{,} \PY{n}{chi2}
         
         \PY{n}{ch2} \PY{o}{=} \PY{n}{SelectKBest}\PY{p}{(}\PY{n}{chi2}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{l+m+mi}{29000}\PY{p}{)}
         
         \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{ch2}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}dev} \PY{o}{=} \PY{n}{ch2}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}dev}\PY{p}{)}
         \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{ch2}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\end{Verbatim}

    Se utiliza la extracción de caracteristicas KBest mediante la metrica
chi cuadrado. Esto ya que documentación de sklearn muestra que es
utilizada cuando la data esta basada en texto (como es el caso).

La cantidad de catacteristicas se obtiene mediante prueba y error.

Fuente:
http://scikit-learn.org/stable/auto\_examples/text/document\_classification\_20newsgroups.html\#example-text-document-classification-20newsgroups-py

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{k+kn}{import} \PY{n+nn}{sklearn.linear\PYZus{}model} \PY{k+kn}{as} \PY{n+nn}{lm}
         
         \PY{n}{model} \PY{o}{=} \PY{n}{lm}\PY{o}{.}\PY{n}{LinearRegression}\PY{p}{(}\PY{n}{fit\PYZus{}intercept} \PY{o}{=} \PY{n+nb+bp}{False}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         
         \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{R2=}\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{model}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
R2=0.600107

    \end{Verbatim}

    Con una regresión linear ordinaria se logra un R² de 0.6.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{k+kn}{import} \PY{n+nn}{sklearn.linear\PYZus{}model} \PY{k+kn}{as} \PY{n+nn}{lm}
         
         \PY{n}{model} \PY{o}{=} \PY{n}{lm}\PY{o}{.}\PY{n}{LassoCV}\PY{p}{(}\PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{7}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         
         \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{R2=}\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{model}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
R2=0.504787

    \end{Verbatim}

    Se prueba además con Lasso pero se obtiene peor resultados. Normalizar
los datos no cambia en nada el \(R^2\). Se utiliza la clase LassoCV que
recorre varios valores de alpha y elije el mejor en el conjunto de
entrenamiento. El alpha de Lasso es:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{n}{model}\PY{o}{.}\PY{n}{alpha\PYZus{}}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}22}]:} 15405802.603399234
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{k+kn}{import} \PY{n+nn}{sklearn.linear\PYZus{}model} \PY{k+kn}{as} \PY{n+nn}{lm}
         
         \PY{n}{model} \PY{o}{=} \PY{n}{lm}\PY{o}{.}\PY{n}{RidgeCV}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         
         \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{R2=}\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{model}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
R2=0.563866

    \end{Verbatim}

    Tampoco tenemos suerte con Ridge.

El alpha de Ridge es:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{model}\PY{o}{.}\PY{n}{alpha\PYZus{}}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor} }]:} 10.0
\end{Verbatim}
        
    Se prueba con ElasticNet que mezcla ambos modelos:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k+kn}{import} \PY{n+nn}{sklearn.linear\PYZus{}model} \PY{k+kn}{as} \PY{n+nn}{lm}
        
        \PY{n}{model} \PY{o}{=} \PY{n}{lm}\PY{o}{.}\PY{n}{ElasticNetCV}\PY{p}{(}\PY{n}{l1\PYZus{}ratio}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,} \PY{n}{alphas}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{7}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        
        \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
        
        \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{R2=}\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{model}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/usr/lib64/python2.7/site-packages/sklearn/linear\_model/coordinate\_descent.py:466: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations
  ConvergenceWarning)
{\ldots}
    \end{Verbatim}

    Y luego se ejecuta en un rango más acotado conforme a los optimos
obtenidos anteriormente:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k+kn}{import} \PY{n+nn}{sklearn.linear\PYZus{}model} \PY{k+kn}{as} \PY{n+nn}{lm}
        
        \PY{n}{model} \PY{o}{=} \PY{n}{lm}\PY{o}{.}\PY{n}{ElasticNetCV}\PY{p}{(}\PY{n}{l1\PYZus{}ratio}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{)}\PY{p}{,} \PY{n}{alphas}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mf}{0.8}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{)}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{7}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        
        \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
        
        \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{R2=}\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{model}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
\end{Verbatim}

    Se obtienen el mejor resultado hasta el momento.

A continuación en vez de realizar cross-validation se usa el conjunto de
validación (``dev'') y modificando los parametros manualmente:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k+kn}{import} \PY{n+nn}{sklearn.linear\PYZus{}model} \PY{k+kn}{as} \PY{n+nn}{lm}
        
        \PY{n}{model} \PY{o}{=} \PY{n}{lm}\PY{o}{.}\PY{n}{ElasticNet}\PY{p}{(}\PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.05}\PY{p}{,} \PY{n}{l1\PYZus{}ratio}\PY{o}{=}\PY{l+m+mf}{0.775}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
        
        \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{R2=}\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{model}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{y\PYZus{}dev}\PY{p}{)}
\end{Verbatim}

    Dando:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{R2=}\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{model}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
\end{Verbatim}

    Cabe mencionar que se probo con tecnicas como Grid Search y Randomized
Parameter Optimization pero sus tiempos de computo fueron excesivos.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{p}{(}\PY{l+m+mf}{0.634180}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mf}{0.5}
\end{Verbatim}

    Esto implica un R de 0.796 aproximadamente.

    \section{Conclusiones}\label{conclusiones}

    En regresiones donde los datos tienen múltiples dimensiones es
fundamental tener mecanismos de reducir la dimensionalidad. Esto se
puede realizar de forma directa (mediante agregar o remover variables) o
indirecta (mediante Lasso o Ridge). Elegir el algoritmo depende de los
datos y que tan agresivo queremos que sea la reducción, entendiendo que
Lasso elimina más repentinamente dimensiones que Ridge.

Técnicas combinadas que ensamblen diferentes algoritmos dan mejores
resultados, al menos en el caso estudiado, que las técnicas
individuales.

La validación de los modelos que uno genera es importante para evitar
falsas ilusiones de éxito. Técnicas como cross validation ayudan a
entender la sensibilidad en el modelo dada una variación en el conjunto
de entrenamiento.

Además es fundamental la elección de parámetros. Elegir la mejor
combinación de estos para un algoritmo puede significar mejoras
importantes en la calidad del resultado y se presenta como un problema
abierto de investigación.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} 
\end{Verbatim}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}

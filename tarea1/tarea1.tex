
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font than Computer Modern for most use cases
    \usepackage{palatino}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Tarea \# 1 MÃ¡quinas de Aprendizaje}
    	\author{Rafik Mas'ad\\Alejandro Sazo}
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Tarea 1 MÃ¡quinas de
Aprendizaje}\label{tarea-1-muxe1quinas-de-aprendizaje}

    \subsection{Ejercicio 1}\label{ejercicio-1}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\itemsep1pt\parskip0pt\parsep0pt
\item
  ConstrucciÃ³n del dataframe. La primera columna del dataframe original
  es redundante para la indexaciÃ³n, mientras que la columna Train nos
  permite identificar cuales ejemplos serÃ¡n parte del training set
  (Train = T) y del testing set (Train = F). Para explicitar quÃ©
  ejemplos son del testing set se invierten los valores de verdad de
  dicha columna. Finalmente la columna ya utilizada se descarta para
  quedarnos con las columnas de predictores.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k+kn}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k+kn}{as} \PY{n+nn}{pd}
        
        \PY{n}{url} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{http://statweb.stanford.edu/\PYZti{}tibs/ElemStatLearn/datasets/prostate.data}\PY{l+s+s1}{\PYZsq{}}
        \PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{n}{url}\PY{p}{,} \PY{n}{sep}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{header}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Remover columna con indices redundantes}
        \PY{n}{df} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Unnamed: 0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Obtener columna con la etiqueta Train y reemplazar valores booleanos. Estos ejemplos seran de entrenamiento}
        \PY{n}{istrain\PYZus{}str} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{istrain} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{p}{[}\PY{n+nb+bp}{True} \PY{k}{if} \PY{n}{s} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{T}\PY{l+s+s1}{\PYZsq{}} \PY{k}{else} \PY{n+nb+bp}{False} \PY{k}{for} \PY{n}{s} \PY{o+ow}{in} \PY{n}{istrain\PYZus{}str}\PY{p}{]}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Listar como testing el resto de valores falsos de la columna anterior}
        \PY{n}{istest} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{logical\PYZus{}not}\PY{p}{(}\PY{n}{istrain}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Una vez procesado los datos, eliminar la columna train para almacenar los predictores relevantes}
        \PY{n}{df} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\itemsep1pt\parskip0pt\parsep0pt
\item
  DescripciÃ³n del dataset. El dataset posee 9 atributos y 97 samples con
  valores enteros y reales.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{n}{df}\PY{o}{.}\PY{n}{shape}
        \PY{n}{df}\PY{o}{.}\PY{n}{info}\PY{p}{(}\PY{p}{)}
        \PY{n}{df}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
<class 'pandas.core.frame.DataFrame'>
Int64Index: 97 entries, 0 to 96
Data columns (total 9 columns):
lcavol     97 non-null float64
lweight    97 non-null float64
age        97 non-null int64
lbph       97 non-null float64
svi        97 non-null int64
lcp        97 non-null float64
gleason    97 non-null int64
pgg45      97 non-null int64
lpsa       97 non-null float64
dtypes: float64(5), int64(4)
memory usage: 7.6 KB

    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}2}]:}           lcavol    lweight        age       lbph        svi        lcp  \textbackslash{}
        count  97.000000  97.000000  97.000000  97.000000  97.000000  97.000000   
        mean    1.350010   3.628943  63.865979   0.100356   0.216495  -0.179366   
        std     1.178625   0.428411   7.445117   1.450807   0.413995   1.398250   
        min    -1.347074   2.374906  41.000000  -1.386294   0.000000  -1.386294   
        25\%     0.512824   3.375880  60.000000  -1.386294   0.000000  -1.386294   
        50\%     1.446919   3.623007  65.000000   0.300105   0.000000  -0.798508   
        75\%     2.127041   3.876396  68.000000   1.558145   0.000000   1.178655   
        max     3.821004   4.780383  79.000000   2.326302   1.000000   2.904165   
        
                 gleason       pgg45       lpsa  
        count  97.000000   97.000000  97.000000  
        mean    6.752577   24.381443   2.478387  
        std     0.722134   28.204035   1.154329  
        min     6.000000    0.000000  -0.430783  
        25\%     6.000000    0.000000   1.731656  
        50\%     7.000000   15.000000   2.591516  
        75\%     7.000000   40.000000   3.056357  
        max     9.000000  100.000000   5.582932  
\end{Verbatim}
        
    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\itemsep1pt\parskip0pt\parsep0pt
\item
  NormalizaciÃ³n de datos. Este preprocesamiento de los datos es
  importante pues las features originales pueden venir en distintas
  escalas por lo tanto nuestro algoritmo de aprendizaje no funcionarÃ¡
  correctamente, por ejemplo al utilizar funciones objetivo que incluyan
  mÃ©tricas, los datos con mayor rango tenderÃ¡n a dominar sobre los de
  menor rango; por otra parte tambiÃ©n podrÃ­a darse el caso de que la
  convergencia en algoritmos que usen gradiente descendiente sea lenta o
  imprecisa.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.preprocessing} \PY{k+kn}{import} \PY{n}{StandardScaler}
        \PY{c+c1}{\PYZsh{} Por defecto centra y escala la data.}
        \PY{n}{scaler} \PY{o}{=} \PY{n}{StandardScaler}\PY{p}{(}\PY{n}{with\PYZus{}mean}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{with\PYZus{}std}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
        \PY{n}{df\PYZus{}scaled} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{scaler}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{df}\PY{p}{)}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{df}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
        \PY{c+c1}{\PYZsh{}  Deseamos aprender a predecir el feature lpsa, por lo que la recuperamos del original}
        \PY{n}{df\PYZus{}scaled}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lpsa}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lpsa}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{3}
\itemsep1pt\parskip0pt\parsep0pt
\item
  RegresiÃ³n lineal con MÃ­nimos Cuadrados. En primer lugar extraemos la
  Ãºltima columna de los datos, que corresponde al output \(y\) de cada
  ejemplo. La nueva columna aÃ±adida corresponde al bias. El argumento
  pasado al constructor de LinearRegression indica que no se calcularÃ¡
  intercepto para el modelo (ya lo hemos hecho a travÃ©s de normalizar e
  ingresar la columna con bias 1)
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k+kn}{import} \PY{n+nn}{sklearn.linear\PYZus{}model} \PY{k+kn}{as} \PY{n+nn}{lm}
        \PY{n}{X} \PY{o}{=} \PY{n}{df\PYZus{}scaled}\PY{o}{.}\PY{n}{ix}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{c+c1}{\PYZsh{} Agregamos la columna de bias con 1}
        \PY{n}{N} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{X}\PY{o}{.}\PY{n}{insert}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{intercept}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{n}{N}\PY{p}{)}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Obtener los datos de output conocidos y extraer test \PYZam{} training set}
        \PY{n}{y} \PY{o}{=} \PY{n}{df\PYZus{}scaled}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lpsa}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{Xtrain} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{istrain}\PY{p}{]}
        \PY{n}{ytrain} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{istrain}\PY{p}{]}
        \PY{n}{Xtest} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{istest}\PY{p}{]}
        \PY{n}{ytest} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{istest}\PY{p}{]}
        \PY{n}{linreg} \PY{o}{=} \PY{n}{lm}\PY{o}{.}\PY{n}{LinearRegression}\PY{p}{(}\PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
        \PY{n}{linreg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xtrain}\PY{p}{,} \PY{n}{ytrain}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}4}]:} LinearRegression(copy\_X=True, fit\_intercept=False, n\_jobs=1, normalize=False)
\end{Verbatim}
        
    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{4}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Tabla de pesos (coeficientes) y Z-scores para cada variable. Los
  Z-scores miden el efecto de descartar alguna variable del modelo.
  Cuando deseamos un 5\% de significancia, las variables que resultarÃ¡n
  mÃ¡s importantes deberÃ¡n tener Z-score en valor abosluto mayor que 2,
  lo que indica una significancia del 5\%. Las variables mas importante
  por lo tanto son en orden de Z-score decreciente \textbf{lcavol},
  \textbf{svi}, \textbf{lcp} y \textbf{pgg45}
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{c+c1}{\PYZsh{} Correlacion entre variables}
        \PY{c+c1}{\PYZsh{} print Xtrain.drop(\PYZsq{}intercept\PYZsq{}, axis=1).corr()}
        
        \PY{c+c1}{\PYZsh{} Tabla con coefficientes y sus Z\PYZhy{}score}
        \PY{n}{coeffs} \PY{o}{=} \PY{n}{linreg}\PY{o}{.}\PY{n}{coef\PYZus{}}
        \PY{n}{Table} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{coeffs}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Coefficent}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        \PY{n}{Table}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Std. Error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{Xtrain}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{Xtrain}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        \PY{n}{Table}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Z\PYZhy{}score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{Table}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Coefficent}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{div}\PY{p}{(}\PY{n}{Table}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Std. Error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
        \PY{n}{Table}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}5}]:}            Coefficent  Std. Error   Z-score
        lcavol       0.676016    0.129469  5.221460
        lweight      0.261694    0.136618  1.915519
        age         -0.140734    0.123746 -1.137281
        lbph         0.209061    0.123892  1.687447
        svi          0.303623    0.124582  2.437133
        lcp         -0.287002    0.123022 -2.332923
        gleason     -0.021195    0.120547 -0.175822
        pgg45        0.265576    0.127584  2.081583
        intercept    2.464933    0.000000       inf
\end{Verbatim}
        
    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{5}
\itemsep1pt\parskip0pt\parsep0pt
\item
  EstimaciÃ³n de errores de predicciÃ³n. El uso de cross validation nos
  permitirÃ¡ entender que tan bien generaliza nuestra mÃ¡quina mientras no
  tengamos disponible el testing set. La mÃ¡quina entrenando con 5 folds
  presenta un MSE (Mean squared error o error cuadrÃ¡tico medio) de 0.95,
  mientras que con 10 folds el MSE disminuye a 0.75. El MSE en el
  testing set es finalmente de 0.5, lo cual es positivo pues nuestro
  modelo original tenÃ­a un alto error en el training set, pero las
  pruebas sobre el testing set fueron mejores.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{c+c1}{\PYZsh{} Predecir en el testing set}
        \PY{n}{yhat\PYZus{}test} \PY{o}{=} \PY{n}{linreg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{Xtest}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Calcular error cuadrÃ¡tico medio en el testing set}
        \PY{n}{mse\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{yhat\PYZus{}test} \PY{o}{\PYZhy{}} \PY{n}{ytest}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
        \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE para testing set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{mse\PYZus{}test}
        \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k+kn}{import} \PY{n}{cross\PYZus{}validation}
        \PY{n}{Xm} \PY{o}{=} \PY{n}{Xtrain}\PY{o}{.}\PY{n}{as\PYZus{}matrix}\PY{p}{(}\PY{p}{)}
        \PY{n}{ym} \PY{o}{=} \PY{n}{ytrain}\PY{o}{.}\PY{n}{as\PYZus{}matrix}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Definir numero de folds}
        \PY{n}{n\PYZus{}folds} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}
        \PY{c+c1}{\PYZsh{} Estimar error con 5 y 10 folds}
        \PY{k}{for} \PY{n}{nf} \PY{o+ow}{in} \PY{n}{n\PYZus{}folds}\PY{p}{:}
            \PY{n}{k\PYZus{}fold} \PY{o}{=} \PY{n}{cross\PYZus{}validation}\PY{o}{.}\PY{n}{KFold}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Xm}\PY{p}{)}\PY{p}{,} \PY{n}{nf}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} MSE para cross validation}
            \PY{n}{mse\PYZus{}cv} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{k}{for} \PY{n}{k}\PY{p}{,} \PY{p}{(}\PY{n}{train}\PY{p}{,} \PY{n}{val}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{k\PYZus{}fold}\PY{p}{)}\PY{p}{:}
                \PY{n}{linreg} \PY{o}{=} \PY{n}{lm}\PY{o}{.}\PY{n}{LinearRegression}\PY{p}{(}\PY{n}{fit\PYZus{}intercept} \PY{o}{=} \PY{n+nb+bp}{False}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} Modelar con el subconjunto del training set dado por el fold}
                \PY{n}{linreg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xm}\PY{p}{[}\PY{n}{train}\PY{p}{]}\PY{p}{,} \PY{n}{ym}\PY{p}{[}\PY{n}{train}\PY{p}{]}\PY{p}{)}
                \PY{n}{yhat\PYZus{}val} \PY{o}{=} \PY{n}{linreg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{Xm}\PY{p}{[}\PY{n}{val}\PY{p}{]}\PY{p}{)}
                \PY{n}{mse\PYZus{}fold} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{yhat\PYZus{}val} \PY{o}{\PYZhy{}} \PY{n}{ym}\PY{p}{[}\PY{n}{val}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                \PY{n}{mse\PYZus{}cv} \PY{o}{+}\PY{o}{=} \PY{n}{mse\PYZus{}fold}
            \PY{n}{mse\PYZus{}cv} \PY{o}{=} \PY{n}{mse\PYZus{}cv} \PY{o}{/} \PY{n}{nf}
            \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE para}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{nf}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{folds:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{mse\PYZus{}cv}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
MSE para testing set: 0.521274005508
MSE para 5 folds: 0.956514631616
MSE para 10 folds: 0.757237472963

    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{9}
\itemsep1pt\parskip0pt\parsep0pt
\item
  QQ Plot para error de prueba. Podemos observar que \(R^2 = 0.9537\),
  lo que indica una alta correlaciÃ³n entre los datos de predicciÃ³n, por
  lo que es razonable suponer que los residuos se distribuyen de forma
  normal.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        \PY{k+kn}{import} \PY{n+nn}{matplotlib.pyplot} \PY{k+kn}{as} \PY{n+nn}{plt}
        \PY{k+kn}{import} \PY{n+nn}{scipy.stats} \PY{k+kn}{as} \PY{n+nn}{stats}
        \PY{n}{residual} \PY{o}{=} \PY{n}{yhat\PYZus{}test} \PY{o}{\PYZhy{}} \PY{n}{ytest}
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
        \PY{n}{stats}\PY{o}{.}\PY{n}{probplot}\PY{p}{(}\PY{n}{residual}\PY{p}{,} \PY{n}{dist}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{norm}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{plot}\PY{o}{=}\PY{n}{plt}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Q\PYZhy{}Q Plot}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Quantiles}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Residuos}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{tarea1_files/tarea1_15_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Ejercicio 2}\label{ejercicio-2}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\itemsep1pt\parskip0pt\parsep0pt
\item
  ImplementaciÃ³n de FSS. En vez de simplemente usar el error cuadrÃ¡tico
  medio como criterio para aceptar o no una variable se ha implementado
  una versiÃ³n que utiliza el coeficiente \(R^2\). GrÃ¡ficamente los
  errores de entrenamiento siempre decrecen, pero con 5 variables los
  errores en el conjunto de test son del orden de 0.45, aÃ±adir mÃ¡s
  variables aumenta el error de test a partir de ese punto
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k}{def} \PY{n+nf}{fss}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{x\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{names\PYZus{}x}\PY{p}{,} \PY{n}{k} \PY{o}{=} \PY{l+m+mi}{10000}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{        Forward Step\PYZhy{}wise Selection}
        \PY{l+s+sd}{        Args:}
        \PY{l+s+sd}{            x: Training set x}
        \PY{l+s+sd}{            y: Training set y}
        \PY{l+s+sd}{            x\PYZus{}test: Testing set x}
        \PY{l+s+sd}{            y\PYZus{}test: Testing set y}
        \PY{l+s+sd}{            names\PYZus{}x: Labels for training set x}
        \PY{l+s+sd}{            k: Max number of variables}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} Numero de features}
            \PY{n}{p} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
            \PY{n}{k} \PY{o}{=} \PY{n+nb}{min}\PY{p}{(}\PY{n}{p}\PY{p}{,} \PY{n}{k}\PY{p}{)}
            \PY{n}{names\PYZus{}x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{names\PYZus{}x}\PY{p}{)}
            \PY{n}{remaining} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{p}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} Mantener intercepto}
            \PY{n}{selected} \PY{o}{=} \PY{p}{[}\PY{n}{p}\PY{p}{]}
            \PY{n}{current\PYZus{}score} \PY{o}{=} \PY{l+m+mf}{0.0}
            \PY{n}{best\PYZus{}new\PYZus{}score} \PY{o}{=} \PY{l+m+mf}{0.0}
            \PY{n}{nvars} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{n}{training\PYZus{}errors} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{n}{test\PYZus{}errors} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{c+c1}{\PYZsh{} Mientras hayan candidatos y las variables seleccionadas no superen k}
            \PY{k}{while} \PY{n}{remaining} \PY{o+ow}{and} \PY{n+nb}{len}\PY{p}{(}\PY{n}{selected}\PY{p}{)} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{n}{k}\PY{p}{:}
                \PY{n}{score\PYZus{}candidates} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                \PY{c+c1}{\PYZsh{} Por cada variable candidata}
                \PY{k}{for} \PY{n}{candidate} \PY{o+ow}{in} \PY{n}{remaining}\PY{p}{:}
                    \PY{c+c1}{\PYZsh{} Crear un nuevo modelo de regresion}
                    \PY{n}{model} \PY{o}{=} \PY{n}{lm}\PY{o}{.}\PY{n}{LinearRegression}\PY{p}{(}\PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
                    \PY{n}{indexes} \PY{o}{=} \PY{n}{selected} \PY{o}{+} \PY{p}{[}\PY{n}{candidate}\PY{p}{]}
                    \PY{c+c1}{\PYZsh{} Extraer como conjunto de entrenamiento el intercepto }
                    \PY{c+c1}{\PYZsh{} y los valores asociados a las variables elegidas}
                    \PY{n}{x\PYZus{}train} \PY{o}{=} \PY{n}{x}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{indexes}\PY{p}{]}
                    \PY{c+c1}{\PYZsh{} Hacer el fit del modelo y predecir}
                    \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y}\PY{p}{)}
                    \PY{c+c1}{\PYZsh{} Predecir sobre training y test}
                    \PY{n}{\PYZus{}x\PYZus{}test} \PY{o}{=} \PY{n}{x\PYZus{}test}\PY{o}{.}\PY{n}{as\PYZus{}matrix}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{indexes}\PY{p}{]}
                    \PY{n}{yhat\PYZus{}train} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{)}
                    \PY{n}{yhat\PYZus{}test} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{\PYZus{}x\PYZus{}test}\PY{p}{)}
                    \PY{c+c1}{\PYZsh{} Obtener residuos y calcular R\PYZca{}2}
                    \PY{n}{residuals\PYZus{}train} \PY{o}{=} \PY{n}{yhat\PYZus{}train} \PY{o}{\PYZhy{}} \PY{n}{y}
                    \PY{n}{residuals\PYZus{}test} \PY{o}{=} \PY{n}{yhat\PYZus{}test} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}test}
                    \PY{c+c1}{\PYZsh{} Calcular Error de entrenamiento}
                    \PY{n}{training\PYZus{}error} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{residuals\PYZus{}train}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                    \PY{n}{test\PYZus{}error} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{residuals\PYZus{}test}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                    \PY{n}{mean\PYZus{}y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{y}\PY{p}{)}
                    \PY{n}{SS\PYZus{}tot} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{mean\PYZus{}y}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                    \PY{n}{SS\PYZus{}res} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{residuals\PYZus{}train}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                    \PY{n}{R2} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{n}{SS\PYZus{}res}\PY{o}{/}\PY{n}{SS\PYZus{}tot}\PY{p}{)}
                    \PY{n}{score\PYZus{}candidates}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{(}\PY{n}{R2}\PY{p}{,} \PY{n}{candidate}\PY{p}{,} \PY{n}{training\PYZus{}error}\PY{p}{,} \PY{n}{test\PYZus{}error}\PY{p}{)}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} Una vez analizadas las candidatas ordenar scores de mayor a menor}
                \PY{n}{score\PYZus{}candidates}\PY{o}{.}\PY{n}{sort}\PY{p}{(}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} Extraer el elemento de mejor score}
                \PY{n}{best\PYZus{}new\PYZus{}score}\PY{p}{,} \PY{n}{best\PYZus{}candidate}\PY{p}{,} \PY{n}{best\PYZus{}training\PYZus{}error}\PY{p}{,} \PY{n}{best\PYZus{}test\PYZus{}error}\PY{o}{=} \PY{n}{score\PYZus{}candidates}\PY{o}{.}\PY{n}{pop}\PY{p}{(}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} Remover al candidato de la lista de restantes y agregarlo a la lista de seleccionados}
                \PY{n}{remaining}\PY{o}{.}\PY{n}{remove}\PY{p}{(}\PY{n}{best\PYZus{}candidate}\PY{p}{)}
                \PY{n}{selected}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{best\PYZus{}candidate}\PY{p}{)}
                \PY{n}{nvars}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{indexes}\PY{p}{)}\PY{p}{)}
                \PY{n}{training\PYZus{}errors}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{best\PYZus{}training\PYZus{}error}\PY{p}{)}
                \PY{n}{test\PYZus{}errors}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{best\PYZus{}test\PYZus{}error}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} R\PYZca{}2 correspondiente a esta configuracion para testing set}
                \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{selected = }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s2}{ ...}\PY{l+s+s2}{\PYZdq{}}\PY{o}{\PYZpc{}}\PY{k}{names\PYZus{}x}[best\PYZus{}candidate]
                \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{totalvars=}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{, R\PYZca{}2 = }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}}\PY{o}{\PYZpc{}}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{indexes}\PY{p}{)}\PY{p}{,}\PY{n}{best\PYZus{}new\PYZus{}score}\PY{p}{)}
            \PY{k}{return} \PY{n}{selected}\PY{p}{,} \PY{n}{nvars}\PY{p}{,} \PY{n}{training\PYZus{}errors}\PY{p}{,} \PY{n}{test\PYZus{}errors}
        
        \PY{n}{names\PYZus{}regressors} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lcavol}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lweight}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Age}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lbph}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Svi}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lcp}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Gleason}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Pgg45}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
        \PY{n}{selected}\PY{p}{,} \PY{n}{nvars}\PY{p}{,} \PY{n}{training\PYZus{}errors}\PY{p}{,} \PY{n}{test\PYZus{}errors} \PY{o}{=} \PY{n}{fss}\PY{p}{(}\PY{n}{Xm}\PY{p}{,} \PY{n}{ym}\PY{p}{,} \PY{n}{Xtest}\PY{p}{,} \PY{n}{ytest}\PY{p}{,} \PY{n}{names\PYZus{}regressors}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{nvars}\PY{p}{,} \PY{n}{training\PYZus{}errors}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bo\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{nvars}\PY{p}{,} \PY{n}{test\PYZus{}errors}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{} variables}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
selected = Lcavol {\ldots}
totalvars=2, R\^{}2 = 0.537516
selected = Lweight {\ldots}
totalvars=3, R\^{}2 = 0.614756
selected = Svi {\ldots}
totalvars=4, R\^{}2 = 0.637441
selected = Lbph {\ldots}
totalvars=5, R\^{}2 = 0.659176
selected = Pgg45 {\ldots}
totalvars=6, R\^{}2 = 0.666920
selected = Lcp {\ldots}
totalvars=7, R\^{}2 = 0.682807
selected = Age {\ldots}
totalvars=8, R\^{}2 = 0.694258
selected = Gleason {\ldots}
totalvars=9, R\^{}2 = 0.694371

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{tarea1_files/tarea1_18_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\itemsep1pt\parskip0pt\parsep0pt
\item
  ImplementaciÃ³n de BSS. Se mantiene la forma de calcular el score
  anterior. En este caso los resultados obtenidos son consistentes con
  la implementaciÃ³n de FSS. Con este dataset y score es posible apreciar
  que ambos algoritmos llegan a una zona donde los errores de testing
  son inferiores a 0.5 y menores a los errores de entrenamiento (modelo
  2 a 5 variables).
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{k}{def} \PY{n+nf}{bss}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{x\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{names\PYZus{}x}\PY{p}{)}\PY{p}{:}
            \PY{n}{names\PYZus{}x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{names\PYZus{}x}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} Inicialmente no descartamos ningun valor}
            \PY{n}{dropped} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{c+c1}{\PYZsh{} Las variables del modelo son todas las originales}
            \PY{n}{variables} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{x}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
            \PY{n}{nvars} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{n}{training\PYZus{}errors} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{n}{test\PYZus{}errors} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{c+c1}{\PYZsh{} Mientras hayan variables}
            \PY{k}{while} \PY{n+nb}{len}\PY{p}{(}\PY{n}{variables}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{1}\PY{p}{:}
                \PY{n}{scores} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                \PY{c+c1}{\PYZsh{} Por cada variable verificar cual es aquella que menos influye}
                \PY{k}{for} \PY{n}{candidate} \PY{o+ow}{in} \PY{n}{variables}\PY{p}{:}
                    \PY{c+c1}{\PYZsh{} Crear un nuevo modelo de regresion}
                    \PY{n}{model} \PY{o}{=} \PY{n}{lm}\PY{o}{.}\PY{n}{LinearRegression}\PY{p}{(}\PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
                    \PY{n}{indexes} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                    \PY{n}{indexes}\PY{p}{[}\PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{n}{variables} \PY{o}{+} \PY{p}{[}\PY{n}{x}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
                    \PY{c+c1}{\PYZsh{} Remover una variable}
                    \PY{n}{indexes}\PY{o}{.}\PY{n}{remove}\PY{p}{(}\PY{n}{candidate}\PY{p}{)}
                    \PY{n}{x\PYZus{}train} \PY{o}{=} \PY{n}{x}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{indexes}\PY{p}{]}
                    \PY{c+c1}{\PYZsh{} Hacer el fit del modelo y predecir}
                    \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y}\PY{p}{)}
                    \PY{c+c1}{\PYZsh{} Predecir sobre training y test}
                    \PY{n}{\PYZus{}x\PYZus{}test} \PY{o}{=} \PY{n}{x\PYZus{}test}\PY{o}{.}\PY{n}{as\PYZus{}matrix}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{indexes}\PY{p}{]}
                    \PY{n}{yhat\PYZus{}test} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{\PYZus{}x\PYZus{}test}\PY{p}{)}
                    \PY{n}{yhat\PYZus{}train} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{)}
                    \PY{c+c1}{\PYZsh{} Obtener residuos y calcular R\PYZca{}2}
                    \PY{n}{residuals\PYZus{}train} \PY{o}{=} \PY{n}{yhat\PYZus{}train} \PY{o}{\PYZhy{}} \PY{n}{y}
                    \PY{n}{residuals\PYZus{}test} \PY{o}{=} \PY{n}{yhat\PYZus{}test} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}test}
                    \PY{c+c1}{\PYZsh{} Calcular Error de entrenamiento}
                    \PY{n}{training\PYZus{}error} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{residuals\PYZus{}train}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                    \PY{n}{test\PYZus{}error} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{residuals\PYZus{}test}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                    \PY{n}{mean\PYZus{}y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{y}\PY{p}{)}
                    \PY{n}{SS\PYZus{}tot} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{mean\PYZus{}y}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                    \PY{n}{SS\PYZus{}res} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{residuals\PYZus{}train}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                    \PY{c+c1}{\PYZsh{} Calcular R\PYZca{}2 para cada modelo}
                    \PY{n}{R2} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{n}{SS\PYZus{}res}\PY{o}{/}\PY{n}{SS\PYZus{}tot}\PY{p}{)}
                    \PY{n}{scores}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{(}\PY{n}{R2}\PY{p}{,} \PY{n}{candidate}\PY{p}{,} \PY{n}{training\PYZus{}error}\PY{p}{,} \PY{n}{test\PYZus{}error}\PY{p}{)}\PY{p}{)}
                \PY{n}{scores}\PY{o}{.}\PY{n}{sort}\PY{p}{(}\PY{p}{)}
                \PY{n}{best\PYZus{}new\PYZus{}score}\PY{p}{,} \PY{n}{drop\PYZus{}candidate}\PY{p}{,} \PY{n}{best\PYZus{}training\PYZus{}error}\PY{p}{,} \PY{n}{best\PYZus{}test\PYZus{}error} \PY{o}{=} \PY{n}{scores}\PY{o}{.}\PY{n}{pop}\PY{p}{(}\PY{p}{)}
                \PY{c+c1}{\PYZsh{}score\PYZus{}dropped[:] = score\PYZus{}dropped[::\PYZhy{}1]}
                \PY{c+c1}{\PYZsh{}worst\PYZus{}new\PYZus{}score, worst\PYZus{}candidate, worst\PYZus{}training\PYZus{}error, worst\PYZus{}test\PYZus{}error = score\PYZus{}dropped.pop()}
                \PY{n}{variables}\PY{o}{.}\PY{n}{remove}\PY{p}{(}\PY{n}{drop\PYZus{}candidate}\PY{p}{)}
                \PY{n}{dropped}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{drop\PYZus{}candidate}\PY{p}{)}
                \PY{n}{nvars}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{indexes}\PY{p}{)}\PY{p}{)}
                \PY{n}{training\PYZus{}errors}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{best\PYZus{}training\PYZus{}error}\PY{p}{)}
                \PY{n}{test\PYZus{}errors}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{best\PYZus{}test\PYZus{}error}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} R\PYZca{}2 correspondiente a esta configuracion para testing set}
                \PY{k}{if}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{variables}\PY{p}{)} \PY{o}{==} \PY{n}{x}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                    \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{No variables dropped}\PY{l+s+s2}{\PYZdq{}}
                    \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{totalvars=}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{, best R\PYZca{}2 = }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}}\PY{o}{\PYZpc{}}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{indexes}\PY{p}{)}\PY{p}{,} \PY{n}{best\PYZus{}new\PYZus{}score}\PY{p}{)}
                \PY{k}{else}\PY{p}{:}
                    \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dropped = }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s2}{ ...}\PY{l+s+s2}{\PYZdq{}}\PY{o}{\PYZpc{}}\PY{k}{names\PYZus{}x}[drop\PYZus{}candidate]
                    \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{surviving = }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s2}{ ...}\PY{l+s+s2}{\PYZdq{}}\PY{o}{\PYZpc{}}\PY{k}{names\PYZus{}x}[variables]
                    \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{totalvars=}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{, best R\PYZca{}2 = }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}}\PY{o}{\PYZpc{}}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{indexes}\PY{p}{)}\PY{p}{,} \PY{n}{best\PYZus{}new\PYZus{}score}\PY{p}{)}
            \PY{k}{return}  \PY{n}{nvars}\PY{p}{,} \PY{n}{training\PYZus{}errors}\PY{p}{,} \PY{n}{test\PYZus{}errors}
        
        \PY{n}{nvars}\PY{p}{,} \PY{n}{training\PYZus{}errors}\PY{p}{,} \PY{n}{test\PYZus{}errors} \PY{o}{=} \PY{n}{bss}\PY{p}{(}\PY{n}{Xm}\PY{p}{,} \PY{n}{ym}\PY{p}{,} \PY{n}{Xtest}\PY{p}{,} \PY{n}{ytest}\PY{p}{,} \PY{n}{names\PYZus{}regressors}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{invert\PYZus{}xaxis}\PY{p}{(}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{nvars}\PY{p}{,} \PY{n}{training\PYZus{}errors}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bo\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{nvars}\PY{p}{,} \PY{n}{test\PYZus{}errors}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{} variables}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
No variables dropped
totalvars=9, best R\^{}2 = 0.694371
dropped = Gleason {\ldots}
surviving = ['Lcavol' 'Lweight' 'Age' 'Lbph' 'Svi' 'Lcp' 'Pgg45'] {\ldots}
totalvars=8, best R\^{}2 = 0.694258
dropped = Age {\ldots}
surviving = ['Lcavol' 'Lweight' 'Lbph' 'Svi' 'Lcp' 'Pgg45'] {\ldots}
totalvars=7, best R\^{}2 = 0.682807
dropped = Lcp {\ldots}
surviving = ['Lcavol' 'Lweight' 'Lbph' 'Svi' 'Pgg45'] {\ldots}
totalvars=6, best R\^{}2 = 0.666920
dropped = Pgg45 {\ldots}
surviving = ['Lcavol' 'Lweight' 'Lbph' 'Svi'] {\ldots}
totalvars=5, best R\^{}2 = 0.659176
dropped = Lbph {\ldots}
surviving = ['Lcavol' 'Lweight' 'Svi'] {\ldots}
totalvars=4, best R\^{}2 = 0.637441
dropped = Svi {\ldots}
surviving = ['Lcavol' 'Lweight'] {\ldots}
totalvars=3, best R\^{}2 = 0.614756
dropped = Lweight {\ldots}
surviving = ['Lcavol'] {\ldots}
totalvars=2, best R\^{}2 = 0.537516

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{tarea1_files/tarea1_20_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Ejercicio 3}\label{ejercicio-3}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Ajuste un modelo lineal utilizando ``Ridge Regression'', es decir,
  regularizando con la norma L2. Utilice valores del parÃ¡metro de
  regularizaciÃ³n lambda en el rango \([10^{-4}, 10^{-1}]\). Construya un
  grÃ¡fico que muestre los coeficientes obtenidos como funciÃ³n del
  parÃ¡metro de regularizaciÃ³n. Describa lo que observa.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{X} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{intercept}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.linear\PYZus{}model} \PY{k+kn}{import} \PY{n}{Ridge}
         \PY{k+kn}{import} \PY{n+nn}{matplotlib.pylab} \PY{k+kn}{as} \PY{n+nn}{plt}
         
         \PY{n}{Xtrain} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{istrain}\PY{p}{]}
         \PY{n}{ytrain} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{istrain}\PY{p}{]}
         \PY{n}{names\PYZus{}regressors} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lcavol}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lweight}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Age}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lbph}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Svi}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lcp}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Gleason}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Pgg45}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
         \PY{n}{alphas\PYZus{}} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{base}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         \PY{n}{coefs} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{model} \PY{o}{=} \PY{n}{Ridge}\PY{p}{(}\PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,}\PY{n}{solver}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{svd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{a} \PY{o+ow}{in} \PY{n}{alphas\PYZus{}}\PY{p}{:}
             \PY{n}{model}\PY{o}{.}\PY{n}{set\PYZus{}params}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{n}{a}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xtrain}\PY{p}{,} \PY{n}{ytrain}\PY{p}{)}
             \PY{n}{coefs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{)}
         
         \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{y\PYZus{}arr}\PY{p}{,} \PY{n}{label} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{coefs}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{names\PYZus{}regressors}\PY{p}{)}\PY{p}{:}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}}\PY{p}{,} \PY{n}{y\PYZus{}arr}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n}{label}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xscale}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlim}\PY{p}{(}\PY{n}{ax}\PY{o}{.}\PY{n}{get\PYZus{}xlim}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{} reverse axis}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{alpha}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weights}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Regularization Path RIDGE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{tarea1_files/tarea1_24_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Se observa que a mayor alpha el algoritmo es mÃ¡s agresivo en no
considerar variables (darles peso 0). El comportamiento se estabiliza,
en este caso, en \(10^{-1}\) (se hizo experimentos con valores menores
de alpha y es muy similar el resultado).

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Ajuste un modelo lineal utilizando el mÃ©todo ``Lasso'', es decir,
  regularizando con la norma L1. Utilice valores del parÃ¡metro de
  regularizaciÃ³n lambda en el rango \([10^1 , 10^{-2}]\). Para obtener
  el cÃ³digo, modifique las lÃ­neas 7 y 9 del ejemplo anterior. Construya
  un grÃ¡fico que muestre los coeficientes obtenidos como funciÃ³n del
  parÃ¡metro de regularizaciÃ³n. Describa lo que observa. Â¿Es mÃ¡s efectivo
  Lasso para seleccionar atributos?
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.linear\PYZus{}model} \PY{k+kn}{import} \PY{n}{Lasso}
         \PY{k+kn}{import} \PY{n+nn}{matplotlib.pylab} \PY{k+kn}{as} \PY{n+nn}{plt}
         
         \PY{n}{Xtrain} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{istrain}\PY{p}{]}
         \PY{n}{ytrain} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{istrain}\PY{p}{]}
         \PY{n}{names\PYZus{}regressors} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lcavol}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lweight}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Age}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lbph}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Svi}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lcp}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Gleason}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Pgg45}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
         \PY{n}{alphas\PYZus{}} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,}\PY{n}{base}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         \PY{n}{coefs} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{model} \PY{o}{=} \PY{n}{Lasso}\PY{p}{(}\PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{a} \PY{o+ow}{in} \PY{n}{alphas\PYZus{}}\PY{p}{:}
             \PY{n}{model}\PY{o}{.}\PY{n}{set\PYZus{}params}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{n}{a}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xtrain}\PY{p}{,} \PY{n}{ytrain}\PY{p}{)}
             \PY{n}{coefs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{)}
         
         \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{y\PYZus{}arr}\PY{p}{,} \PY{n}{label} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{coefs}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{names\PYZus{}regressors}\PY{p}{)}\PY{p}{:}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}}\PY{p}{,} \PY{n}{y\PYZus{}arr}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n}{label}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xscale}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlim}\PY{p}{(}\PY{n}{ax}\PY{o}{.}\PY{n}{get\PYZus{}xlim}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{} reverse axis}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{alpha}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weights}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Regularization Path LASSO}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{tarea1_files/tarea1_27_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Con Lasso se anulan las variables de manera mÃ¡s acelerada lo cual logra
que la selecciÃ³n del parÃ¡metro alpha se comporte, aproximadamente, como
la cantidad de variables a anular.

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Utilizando ``Ridge Regression'', construya un grÃ¡fico que muestre el
  error de entrenamiento y el error de pruebas como funciÃ³n del
  parÃ¡metro de regularizaciÃ³n. Discuta lo que observa.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{Xtest} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{logical\PYZus{}not}\PY{p}{(}\PY{n}{istrain}\PY{p}{)}\PY{p}{]}
         \PY{n}{ytest} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{logical\PYZus{}not}\PY{p}{(}\PY{n}{istrain}\PY{p}{)}\PY{p}{]}
         \PY{n}{alphas\PYZus{}} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,}\PY{n}{base}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         \PY{n}{coefs} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{model} \PY{o}{=} \PY{n}{Ridge}\PY{p}{(}\PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
         \PY{n}{mse\PYZus{}test} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{mse\PYZus{}train} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         
         \PY{k}{for} \PY{n}{a} \PY{o+ow}{in} \PY{n}{alphas\PYZus{}}\PY{p}{:}
             \PY{n}{model}\PY{o}{.}\PY{n}{set\PYZus{}params}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{n}{a}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xtrain}\PY{p}{,} \PY{n}{ytrain}\PY{p}{)}
             \PY{n}{yhat\PYZus{}train} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{Xtrain}\PY{p}{)}
             \PY{n}{yhat\PYZus{}test} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{Xtest}\PY{p}{)}
             \PY{n}{mse\PYZus{}train}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{yhat\PYZus{}train} \PY{o}{\PYZhy{}} \PY{n}{ytrain}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             \PY{n}{mse\PYZus{}test}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{yhat\PYZus{}test} \PY{o}{\PYZhy{}} \PY{n}{ytest}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}}\PY{p}{,}\PY{n}{mse\PYZus{}train}\PY{p}{,}\PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train error ridge}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}}\PY{p}{,}\PY{n}{mse\PYZus{}test}\PY{p}{,}\PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test error ridge}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xscale}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlim}\PY{p}{(}\PY{n}{ax}\PY{o}{.}\PY{n}{get\PYZus{}xlim}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         
         \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El error minimo se encuentra en alpha =}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{min}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{mse\PYZus{}test}\PY{p}{,} \PY{n}{yhat\PYZus{}test}\PY{p}{)}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{tarea1_files/tarea1_30_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
El error minimo se encuentra en alpha = 2.20307927232

    \end{Verbatim}

    El error de entrenamiento de Ridge es menor a medida que disminuye el
alpha (mÃ¡s variables son consideradas) producto, probablemente, del
sobre ajuste. Mientras tanto el error en el conjunto de prueba encuentra
su optimo en un punto (en este caso en alpha igual a 2.2) lo que hace
latente la necesidad de usar tÃ©cnicas como cross-validation para
empÃ­ricamente tener una mejor intuiciÃ³n de cuales son los parÃ¡metros
Ã³ptimos.

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{3}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Utilizando ``Lasso'', construya un grÃ¡fico que muestre el error de
  entrenamiento y el error de pruebas como funciÃ³n del parÃ¡metro de
  regularizaciÃ³n. Discuta lo que observa.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{Xtest} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{logical\PYZus{}not}\PY{p}{(}\PY{n}{istrain}\PY{p}{)}\PY{p}{]}
         \PY{n}{ytest} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{logical\PYZus{}not}\PY{p}{(}\PY{n}{istrain}\PY{p}{)}\PY{p}{]}
         \PY{n}{alphas\PYZus{}} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,}\PY{n}{base}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         \PY{n}{coefs} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{model} \PY{o}{=} \PY{n}{Lasso}\PY{p}{(}\PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
         \PY{n}{mse\PYZus{}test} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{mse\PYZus{}train} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         
         \PY{k}{for} \PY{n}{a} \PY{o+ow}{in} \PY{n}{alphas\PYZus{}}\PY{p}{:}
             \PY{n}{model}\PY{o}{.}\PY{n}{set\PYZus{}params}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{n}{a}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xtrain}\PY{p}{,} \PY{n}{ytrain}\PY{p}{)}
             \PY{n}{yhat\PYZus{}train} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{Xtrain}\PY{p}{)}
             \PY{n}{yhat\PYZus{}test} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{Xtest}\PY{p}{)}
             \PY{n}{mse\PYZus{}train}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{yhat\PYZus{}train} \PY{o}{\PYZhy{}} \PY{n}{ytrain}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             \PY{n}{mse\PYZus{}test}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{yhat\PYZus{}test} \PY{o}{\PYZhy{}} \PY{n}{ytest}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}}\PY{p}{,}\PY{n}{mse\PYZus{}train}\PY{p}{,}\PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train error lasso}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}}\PY{p}{,}\PY{n}{mse\PYZus{}test}\PY{p}{,}\PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test error lasso}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xscale}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlim}\PY{p}{(}\PY{n}{ax}\PY{o}{.}\PY{n}{get\PYZus{}xlim}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{tarea1_files/tarea1_33_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Se observa lo mismo que el ejercicio anterior pero de forma mÃ¡s agresiva
producto de lo discutido en la pregunta (b)

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{4}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Estime el valor del parÃ¡metro de regularizaciÃ³n en los mÃ©todos
  anteriores usando validaciÃ³n cruzada.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{k}{def} \PY{n+nf}{MSE}\PY{p}{(}\PY{n}{y}\PY{p}{,}\PY{n}{yhat}\PY{p}{)}\PY{p}{:} \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{yhat}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
         \PY{n}{Xm} \PY{o}{=} \PY{n}{Xtrain}\PY{o}{.}\PY{n}{as\PYZus{}matrix}\PY{p}{(}\PY{p}{)}
         \PY{n}{ym} \PY{o}{=} \PY{n}{ytrain}\PY{o}{.}\PY{n}{as\PYZus{}matrix}\PY{p}{(}\PY{p}{)}
         \PY{n}{k\PYZus{}fold} \PY{o}{=} \PY{n}{cross\PYZus{}validation}\PY{o}{.}\PY{n}{KFold}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Xm}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}
         \PY{n}{best\PYZus{}cv\PYZus{}mse} \PY{o}{=} \PY{n+nb}{float}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{inf}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{model} \PY{o}{=} \PY{n}{Lasso}\PY{p}{(}\PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{a} \PY{o+ow}{in} \PY{n}{alphas\PYZus{}}\PY{p}{:}
             \PY{n}{model}\PY{o}{.}\PY{n}{set\PYZus{}params}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{n}{a}\PY{p}{)}
             \PY{n}{mse\PYZus{}list\PYZus{}k10} \PY{o}{=} \PY{p}{[}\PY{n}{MSE}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xm}\PY{p}{[}\PY{n}{train}\PY{p}{]}\PY{p}{,} \PY{n}{ym}\PY{p}{[}\PY{n}{train}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{Xm}\PY{p}{[}\PY{n}{val}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{ym}\PY{p}{[}\PY{n}{val}\PY{p}{]}\PY{p}{)} \PYZbs{}
             \PY{k}{for} \PY{n}{train}\PY{p}{,} \PY{n}{val} \PY{o+ow}{in} \PY{n}{k\PYZus{}fold}\PY{p}{]}
             \PY{k}{if} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{mse\PYZus{}list\PYZus{}k10}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n}{best\PYZus{}cv\PYZus{}mse}\PY{p}{:}
                 \PY{n}{best\PYZus{}cv\PYZus{}mse} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{mse\PYZus{}list\PYZus{}k10}\PY{p}{)}
                 \PY{n}{best\PYZus{}alpha} \PY{o}{=} \PY{n}{a}
         
         \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BEST PARAMETER=}\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{, MSE(CV)=}\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}}\PY{o}{\PYZpc{}}\PY{p}{(}\PY{n}{best\PYZus{}alpha}\PY{p}{,}\PY{n}{best\PYZus{}cv\PYZus{}mse}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
BEST PARAMETER=0.010000, MSE(CV)=0.758661

    \end{Verbatim}

    \section{Ejercicio 4}\label{ejercicio-4}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Lea los archivos de datos y cÃ¡rguelos en dos dataframe o matrices X,
  y. En el caso de X es extremadamente importante que mantenga el
  formato disperso (sparse) (Â¿porquÃ©?).
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{k+kn}{import} \PY{n+nn}{os.path}
         \PY{n}{data\PYZus{}base\PYZus{}path} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./data}\PY{l+s+s2}{\PYZdq{}}
         \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k+kn}{as} \PY{n+nn}{pd}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k+kn}{as} \PY{n+nn}{np}
         \PY{k+kn}{from} \PY{n+nn}{scipy.sparse} \PY{k+kn}{import} \PY{n}{csr\PYZus{}matrix}
         \PY{k+kn}{from} \PY{n+nn}{scipy.io} \PY{k+kn}{import} \PY{n}{mmread}
         
         \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{csr\PYZus{}matrix}\PY{p}{(}\PY{n}{mmread}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{data\PYZus{}base\PYZus{}path}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train.x.mm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{loadtxt}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{data\PYZus{}base\PYZus{}path}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train.y.dat}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{X\PYZus{}dev} \PY{o}{=} \PY{n}{csr\PYZus{}matrix}\PY{p}{(}\PY{n}{mmread}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{data\PYZus{}base\PYZus{}path}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dev.x.mm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{y\PYZus{}dev} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{loadtxt}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{data\PYZus{}base\PYZus{}path}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dev.y.dat}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{csr\PYZus{}matrix}\PY{p}{(}\PY{n}{mmread}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{data\PYZus{}base\PYZus{}path}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test.x.mm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{loadtxt}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{data\PYZus{}base\PYZus{}path}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test.y.dat}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    Se usa una matriz sparse ya que, producto de que hay muchos datos vacios
(normal en texto). AsÃ­ se comprime la matriz y queda en un tamaÃ±o
manejable.

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Construya un modelo lineal que obtenga un coeficiente de determinaciÃ³n
  (sobre el conjunto de pruebas) de al menos 0.75.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{get\PYZus{}shape}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}17}]:} (1147, 145256)
\end{Verbatim}
        
    Se observa que hay 1147 peliculas cada una con 145256 atributos.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{c+c1}{\PYZsh{}from sklearn.preprocessing import StandardScaler}
         
         \PY{c+c1}{\PYZsh{}x\PYZus{}scaler = StandardScaler(with\PYZus{}mean=False)}
         
         \PY{c+c1}{\PYZsh{}X\PYZus{}train = x\PYZus{}scaler.fit\PYZus{}transform(X\PYZus{}train, y\PYZus{}train)}
         \PY{c+c1}{\PYZsh{}X\PYZus{}test = x\PYZus{}scaler.transform(X\PYZus{}test, y\PYZus{}test)}
\end{Verbatim}

    Se prueba escalar los datos pero esto devuelve peores resultados.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.feature\PYZus{}selection} \PY{k+kn}{import} \PY{n}{SelectKBest}\PY{p}{,} \PY{n}{chi2}
         
         \PY{n}{ch2} \PY{o}{=} \PY{n}{SelectKBest}\PY{p}{(}\PY{n}{chi2}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{l+m+mi}{29000}\PY{p}{)}
         
         \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{ch2}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}dev} \PY{o}{=} \PY{n}{ch2}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}dev}\PY{p}{)}
         \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{ch2}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\end{Verbatim}

    Se utiliza la extracciÃ³n de caracteristicas KBest mediante la metrica
chi cuadrado. Esto ya que documentaciÃ³n de sklearn muestra que es
utilizada cuando la data esta basada en texto (como es el caso).

La cantidad de catacteristicas se obtiene mediante prueba y error.

Fuente:
http://scikit-learn.org/stable/auto\_examples/text/document\_classification\_20newsgroups.html\#example-text-document-classification-20newsgroups-py

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{k+kn}{import} \PY{n+nn}{sklearn.linear\PYZus{}model} \PY{k+kn}{as} \PY{n+nn}{lm}
         
         \PY{n}{model} \PY{o}{=} \PY{n}{lm}\PY{o}{.}\PY{n}{LinearRegression}\PY{p}{(}\PY{n}{fit\PYZus{}intercept} \PY{o}{=} \PY{n+nb+bp}{False}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         
         \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{R2=}\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{model}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
R2=0.600107

    \end{Verbatim}

    Con una regresiÃ³n linear ordinaria se logra un RÂ² de 0.6.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{k+kn}{import} \PY{n+nn}{sklearn.linear\PYZus{}model} \PY{k+kn}{as} \PY{n+nn}{lm}
         
         \PY{n}{model} \PY{o}{=} \PY{n}{lm}\PY{o}{.}\PY{n}{LassoCV}\PY{p}{(}\PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{7}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         
         \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{R2=}\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{model}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
R2=0.504787

    \end{Verbatim}

    Se prueba ademÃ¡s con Lasso pero se obtiene peor resultados. Normalizar
los datos no cambia en nada el \(R^2\). Se utiliza la clase LassoCV que
recorre varios valores de alpha y elije el mejor en el conjunto de
entrenamiento. El alpha de Lasso es:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{n}{model}\PY{o}{.}\PY{n}{alpha\PYZus{}}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}22}]:} 15405802.603399234
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{k+kn}{import} \PY{n+nn}{sklearn.linear\PYZus{}model} \PY{k+kn}{as} \PY{n+nn}{lm}
         
         \PY{n}{model} \PY{o}{=} \PY{n}{lm}\PY{o}{.}\PY{n}{RidgeCV}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         
         \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{R2=}\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{model}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
R2=0.563866

    \end{Verbatim}

    Tampoco tenemos suerte con Ridge.

El alpha de Ridge es:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{model}\PY{o}{.}\PY{n}{alpha\PYZus{}}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor} }]:} 10.0
\end{Verbatim}
        
    Se prueba con ElasticNet que mezcla ambos modelos:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k+kn}{import} \PY{n+nn}{sklearn.linear\PYZus{}model} \PY{k+kn}{as} \PY{n+nn}{lm}
        
        \PY{n}{model} \PY{o}{=} \PY{n}{lm}\PY{o}{.}\PY{n}{ElasticNetCV}\PY{p}{(}\PY{n}{l1\PYZus{}ratio}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,} \PY{n}{alphas}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{7}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        
        \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
        
        \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{R2=}\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{model}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/usr/lib64/python2.7/site-packages/sklearn/linear\_model/coordinate\_descent.py:466: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations
  ConvergenceWarning)
{\ldots}
    \end{Verbatim}

    Y luego se ejecuta en un rango mÃ¡s acotado conforme a los optimos
obtenidos anteriormente:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k+kn}{import} \PY{n+nn}{sklearn.linear\PYZus{}model} \PY{k+kn}{as} \PY{n+nn}{lm}
        
        \PY{n}{model} \PY{o}{=} \PY{n}{lm}\PY{o}{.}\PY{n}{ElasticNetCV}\PY{p}{(}\PY{n}{l1\PYZus{}ratio}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{)}\PY{p}{,} \PY{n}{alphas}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mf}{0.8}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{)}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{7}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        
        \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
        
        \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{R2=}\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{model}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
\end{Verbatim}

    Se obtienen el mejor resultado hasta el momento.

A continuaciÃ³n en vez de realizar cross-validation se usa el conjunto de
validaciÃ³n (``dev'') y modificando los parametros manualmente:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k+kn}{import} \PY{n+nn}{sklearn.linear\PYZus{}model} \PY{k+kn}{as} \PY{n+nn}{lm}
        
        \PY{n}{model} \PY{o}{=} \PY{n}{lm}\PY{o}{.}\PY{n}{ElasticNet}\PY{p}{(}\PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.05}\PY{p}{,} \PY{n}{l1\PYZus{}ratio}\PY{o}{=}\PY{l+m+mf}{0.775}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
        
        \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{R2=}\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{model}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{y\PYZus{}dev}\PY{p}{)}
\end{Verbatim}

    Dando:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{R2=}\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{model}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
\end{Verbatim}

    Cabe mencionar que se probo con tecnicas como Grid Search y Randomized
Parameter Optimization pero sus tiempos de computo fueron excesivos.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{p}{(}\PY{l+m+mf}{0.634180}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mf}{0.5}
\end{Verbatim}

    Esto implica un R de 0.796 aproximadamente.

    \section{Conclusiones}\label{conclusiones}

    En regresiones donde los datos tienen mÃºltiples dimensiones es
fundamental tener mecanismos de reducir la dimensionalidad. Esto se
puede realizar de forma directa (mediante agregar o remover variables) o
indirecta (mediante Lasso o Ridge). Elegir el algoritmo depende de los
datos y que tan agresivo queremos que sea la reducciÃ³n, entendiendo que
Lasso elimina mÃ¡s repentinamente dimensiones que Ridge.

TÃ©cnicas combinadas que ensamblen diferentes algoritmos dan mejores
resultados, al menos en el caso estudiado, que las tÃ©cnicas
individuales.

La validaciÃ³n de los modelos que uno genera es importante para evitar
falsas ilusiones de Ã©xito. TÃ©cnicas como cross validation ayudan a
entender la sensibilidad en el modelo dada una variaciÃ³n en el conjunto
de entrenamiento.

AdemÃ¡s es fundamental la elecciÃ³n de parÃ¡metros. Elegir la mejor
combinaciÃ³n de estos para un algoritmo puede significar mejoras
importantes en la calidad del resultado y se presenta como un problema
abierto de investigaciÃ³n.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} 
\end{Verbatim}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
